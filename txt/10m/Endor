








































ENDOR.COIN PROTOCOL

Make Artificial Intelligence
Predictions Accessible for All

Powered by Endor Ltd.

January 11, 2018



Make Artificial Intelligence Predictions Accessible for All

Abstract

Endor.coin is reinventing predictive analytics by democratizing access to Artificial Intelli-
gence data analysis, making it accessible, trustless, censorship-resistance and useful for all.

This is achieved through the Endor.coin protocol, creating the world’s first automated,
self-served, predictive platform that allows business users and unprofessional crypto-token
holders alike to ask complex predictive questions and obtain high-quality results in minutes.
This aims to democratize the field of Data Science, that today is reserved mostly for Fortune-
500 companies. Endor.coin is based on the novel science of Social Physics developed at MIT
by the project’s team members Prof. Alex Pentland and Dr. Yaniv Altshuler.

• Constantly-expanding catalogue of predictions: Endor.coin will be launched
with a variety of pre-defined tokens-related predictions (e.g. tokens predicted to in-
crease volume, decrease volatility, ...). These predictions would be accessible for pur-
chase using the platform’s dedicated EDR token. Endor.coin would then expand the
selection of predictions it caters by allowing users to send Requests For Predictions
(RFP) – suggesting new types of predictions, that would be implemented and become
available for purchase.

• Do-it-yourself API for advanced users: tech-savvy users and professionals would
be provided a self-serve interface, allowing them to easily provide a definition of any
desired behavioral pattern, having the Endor.coin platform automatically generating
a “look-a-likes” prediction of such pattern in return.

• Automatic fusion of private and public data: Endor.coin commercial customers
(such as retail banks, retailers and insurers) will be able to easily integrate their pro-
prietary data streams with the platform – producing high-quality predictive insights
harvested from the fusion of privacy and public data. Thanks to the use of Social
Physics data integration is automatic and friendly – requiring no cleaning or data-
preparation. Private data-streams will be accessible to their owners only.

• Predictions by the People, for the People: using Social Physics data is processed
once, and users pay only for the personalization component they require, offering the
“99%” predictive capability reserved today for tech-giants, for 1% of its cost.

Endor.coin is rooted in the technological platform of Endor. A Gartner Cool Vendor,
recently recognized by the World Economic Forum as a Technology Pioneer, Endor is an
MIT spin-off financially backed by leading investors such as Innovation Endeavors, working
with Fortune-500 companies such as Coca Cola, Walmart and MasterCard.



Contents

1 Value Proposition 4

2 Introduction 6
2.1 An Inefficient and Troubled Market . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Democratization Requires Decentralization and Data Separation . . . . . . . 6
2.3 Technological Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.4 Endor.coin Protocol — Project Overview . . . . . . . . . . . . . . . . . . . . 8

3 Democratizing the Prediction Science 10
3.1 Social Physics – a New Science from MIT . . . . . . . . . . . . . . . . . . . 10
3.2 1st Phase : Endor.com – Automatic Prediction Engine for Enterprizes . . . 15
3.3 2nd Phase : Endor.coin Protocol – Data Science for the Masses . . . . . . 17

3.3.1 Data Providers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.3.2 Prediction Engines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.3.3 Pre-Defined Predictions and Request for Predictions (RFPs) . . . . . 18
3.3.4 Private-Data Analysis and Self-Serve API . . . . . . . . . . . . . . . 18

3.4 Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

4 Trustless, Censorship-Resistant and Accountable 21
4.1 Accountability and Authenticity for Artificial Intelligence . . . . . . . . . . . 21
4.2 Censorship-Resistance through a Decentralized Protocol . . . . . . . . . . . 22
4.3 Network Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

5 Token Implementation 26
5.1 Blockchain Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.2 Smart Contracts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5.3 Catalysts – Application Developers . . . . . . . . . . . . . . . . . . . . . . . 28
5.4 Data Sovereignty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

6 Technological Advantages and Differentiation 31
6.1 A Scientific Revolution from the MIT Furnaces . . . . . . . . . . . . . . . . 31
6.2 Real Product. Proven Technology . . . . . . . . . . . . . . . . . . . . . . . . 31
6.3 Usability, Value to Users and Value for Token Holders . . . . . . . . . . . . . 32

2



7 Team 33
7.1 Key Team Members . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
7.2 Advisors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

Appendix A : Social Physics Explained 38

Appendix B : Endor’s Common Use-Cases for Enterprizes 74

Appendix C : Endor.coin Examples of Pre-Defined Predictions 86

Appendix D : Knowledge Sphere Class API 87

Bibliography 94

3



Chapter 1

Value Proposition

The ability to understand, predict and influence consumer behavior quickly could give any
business an unfair advantage over its competition. Smart business leaders have many ideas
for influencing customer behavior to improve business performance. To implement them,
they need to answer questions such as:

• Who are our top customers and how do we acquire more of them?

• Who is likely to try this newly-launched product?

• How can we reduce our reliance on promotions?

• Where should we open our next store?

• Who will switch from product A to B next month?

To answer these questions, organizations turn to powerful tools like data science and
predictive analytics. Unfortunately, the current process for implementing these tools is slow,
painful, and expensive.

• Requires ‘unicorns’: well-trained, expensive, rare data scientists and PhDs

• Requires 4-6 iterations, each taking a few days / weeks

• Each new business question requires building a new model over a few weeks

• When products and behaviors change, the model breaks

Powered by MIT’s novel Social Physics technology the Endor.coin blockchain-
based protocol is the first decentralized, trustless, censorship resistance behav-
ioral prediction platform that provides high-quality results for any predictive
question in minutes. No coding, data cleaning, or a team of PhDs required.

4



Following are the key aspects of the Endor.coin project:

• Technology: Powered by MIT Social Physics technology [1], providing up to X10
higher accuracy for trends prediction (see our academic work on trends prediction in
financial markets [2–6], relevant patents [7, 8] and additional reviews [9–11]).

• Innovation: Focuses on the automatic modeling of short and medium-range behav-
ioral patterns (days to weeks), detecting such signals before they become observable by
any other available technology (see Dr. Altshuler’s talk at FirstMark ’s “Data-Driven
New York” talk [12], or a large-scale real-time analysis of financial investments [13]).

• Industry Validation: Endor.coin is based on the technology developed by Endor.com
– an MIT spin-off [14], financially backed by leading investors [15], working with For-
tune 500 companies such as Coca Cola [16], MasterCard [17], Walmart and others.
See Endor’s product featured at Finnovate 2017 [18].

• Awards and Recognition: Endor is a Gartner Cool Vendor [19], and was recognized
by the World Economic Forum as a “Technological Pioneer” [20]. The research done
by the project’s team at MIT had led to winning several additional prizes such as the
prestigious DARPA Network Challenge [21], and the McKinsey Award [22].

• Team: Spearheaded by a team of world experts in blockchain, digital banking tech-
nologies and predictive analytics from MITs Computer Science and Artificial Intelli-
gence Lab, the MIT Media Lab and MIT Sloan School of Management. Prof. Pentland
(co-founder), a member of the U.S. National Academy of Engineering, is one of the
world’s most-cited scientists [23], and was recently declared by Forbes as the “7 most
powerful data scientists in the world” [24]. The team has collectively published hun-
dreds of scientific papers, dozens of commercial patents, as well as 8 books dedicated
to blockchain, machine intelligence, and data privacy [25–32].

• Reinventing Predictive Analytics: After years where Artificial Intelligence and
Machine Learning capabilities were reserved for deep-pocketed companies, Endor.coin
offers individuals and small businesses access to superior tech, for a fraction of the
cost. Predictive insights are based on the collective analysis of the contributed data,
offered at a low cost, while allowing data owners to control the privacy of their data.
The Endor.coin protocol enables the integration of new data sources, as well as new
prediction engines, creating a double network effect – the more players join, cost per
prediction decreases, while increasing prediction accuracy.

• Trustless, Fully Decentralized, Accountable and Censorship Resistance: The
Endor.coin protocol is fully-decentralized, providing complete accountability for the
prediction results. This prevents any manipulation or bias during of the predictions.
In addition, the decentralized and open nature of the protocol enables the support of
any prediction, preventing censorship by any single point of authority.

5



Chapter 2

Introduction

2.1 An Inefficient and Troubled Market

In our world there many are organizations that gathers, possess and carefully maintain data.
There are Data Scientists and Machine Learning experts, who can process data and build
predictive models. And, there are many people with a desire to predict the future (these
range from high ranking executives, through middle range marketing or product managers
in large companies, through individuals who try to decide on the right timing to buy a plane
ticket). Today, in order for the latter to be offered predictions for their questions, all three
entities must co-exist in the same organization. This means that 99% of the predictions
are being generated by, and for, stakeholders in large companies. Furthermore, this process
today is highly expensive, as Data Scientists are rare and expensive, and the process of
producing predictions often require months of hard-work, dedicated to every single project.
This sets a very high entry barrier (and price tag) for anyone who is interested in prediction.

2.2 Democratization Requires Decentralization and Data

Separation

The democratization of Artificial Intelligence and Predictive Analytics, making it accessi-
ble for the ‘everyman’, requires the development of a new paradigm, that would meet the
following requirements:

Separation: apart from large corporations and research labs, there is almost no commercial,
academic or non-for-profit organization that can sustain high-quality activities that
combines data curation, data science and the generation of semantic oriented questions.
Individuals, NGOs and small-to-medium business usually only focus on one of those
dimensions – they either possess (or produce) carefully maintained data, employ strong
(and expensive) data scientists, or are experts in ‘asking the right questions’. Therefore,
in order to open the bottleneck and make predictions accessible outside the Fortune-500
club, These three basic elements need to be inherently separated: a truly democratized

6



prediction protocol must allow data-providers to freely contribute data (either marked
public or private), while allowing technology experts to contribute AI and Prediction
engines (that would seamlessly be plugged in, and integrated with the protocol), all
of this – to allow the end-users to easily consume predictions that are based on these
data sources, and prepared for them by these engines.

Accountability: in a data science department of a tech-giant accountability is not required
as a key feature, as it is automatically provided by the fact that “everyone is working
for the same boss” (e.g. the relevant C-level executive in charge of business intelligence,
marketing prediction, and so on). In a democratized platform, where data, intelligence
and computation is constantly being rented on an ad-hoc basis, accountability becomes
essential, for guaranteeing ‘fair play’, and tuning the merit function of all stakeholders
for the ‘long terms’ rather then encouraging short-term revenues.

Decentralization: There are two critical contributions to a decentralized prediction frame-
work – one being engineering related, and the other revolves around censorship re-
sistance and bias prevention. As demonstrated in many examples, a decentralized
solution tend to be easier to scale as well as to extend. Adding more data sources,
computational resources and different types of prediction engines – all greatly benefit
from a decentralized solution. In addition, Decentralized architecture is the only one
that guarantees that these predictions would not be subject to explicit censorship, or
implicit one, that is executed through the generation of biased results, or arbitration
of monetary resources. Furthermore, efficient decentralization is the key to emerging
network effects, that push cost-per-user down, while constantly increasing prediction
accuracy, with the increase of the numbers of participants (both data-providers, pre-
diction engines, and prediction consumers).

2.3 Technological Gap

Unfortunately, although the benefits of Data Science democratization have been clear for
quite some time, implementing such a framework in reality has been challenging, to say the
least. The main reasons is technological – the prevailing science of today simply cannot
support a “generic decentralized behavioral prediction” paradigm... The technologies that
exist today, be it Neural Networks (or Deep Learning), Genetic Programming, Decision
Forests, SVMs and so on – all require a massive amount of data sanitation, processing and
understanding, before any ‘real’ machine learning work can commence. This is the source of
the bottleneck that the industry faces today, the rise of skilled Data Scientists’ salaries, and
their scarcity. A detailed discussion on this topic appears in Section 3.1 and Appendix A.

Without a scientific breakthrough that can automatically digest any data, allowing non-
professionals and professionals alike to ask any predictive question – the industry was de-
tained to the existing paradigm, heavily bottlenecked by the pace a company hires new data
scientists, and the 6-figures salaries it is willing to pay them.

7



2.4 Endor.coin Protocol — Project Overview

In order to transcend these limitations, a new Science had to be developed. Social Physics,
developed by Endor.coin founders Dr. Yaniv Altshuler and Prof. Alex “Sandy” Pentland,
is a mathematical theory that efficiently models the way human crowds behave. Through
a set of mathematical equations that are shown to emerge in behavioral data sources, the
Social Physics theory enables the automatic transformation of any behavioral data source
to a set of behavioral clusters. This requires no cleaning, pre-processing, or understanding
of the semantics of the data (or the questions to be asked). This collection of behavioral
clusters is known as the “Knowledge Sphere”.

The Endor.coin protocol is based on the fact that when behavioral data is being handled
in a canonic representation of behavioral clusters, the traditional process of Data Science can
(finally) be broken down into it basic components, allocating each of them, in a decentralized
way, to different executors. Following is the basic outline and main components of the
Endor.coin protocol:

Canonic Data Representation: Every data that is contributed to the Endor.coin net-
work gets transformed to the “Knowledge Sphere” canonical representation. This can
be done by the various Predictions Engines (see below), and the execution cost is paid
by the engines. Once data undergoes this transformation the various behavioral clus-
ters extracted from it can then be bundled together with clusters from other types of
data, resulting in an efficient and automatic prediction process (see more details in
Section 3.1 and Appendix A).

Separation of Data Providers: As data gets transformed into the canonic “Knowledge
Sphere” representation, data providers are no longer required to actively take part in
the later phases of the analysis. This enables data-owners to integrate their data (in
full or in part) with the Endor.coin network, acting as an autonomous stakeholder in
the ecosystem – focusing on maintaining the quality of their data, controlling who will
have access to which part of it, and benefiting financially from future value it provides.

Separation of Prediction Engines: A known secret among data scientists is that around
90% of the time spent in a data science project is spent on data sanitation and pre-
processing. Due to the revolutionary aspect Social Physics that for the first time
automates these steps – various prediction engines can finally be seamlessly connected
to data sources of various types. The only thing required for a provider of a prediction
engine in order to connect to the Endor.coin network is to support the Endor.coin
protocol – defined as the ability to digest datasets (selectively, as defined by the engine),
and provide output in the form of a “Knowledge Sphere” (see complete specification
and API code in Appendix D).

Decentralized Execution: Using Blockchain. data-providers can donate data (stored on
AWS) and accessible using the Endor.coin protocol. Extraction of behavioral clus-
ters is done in a decentralized way by the various Prediction Engines. Queries are

8



being triggered by end-users by requesting the Endor.coin smart contract to issue a
certain prediction (for EDR fee). For each prediction the best behavioral cluster are
chosen by the Endor.coin prediction code (can be freely accessible on the project’s
GIT account [33]). Funds arbitration is being taken of by the smart contract among
data contributors and prediction engines, according to the clusters chosen for that
prediction. This arbitration optimizes quality of prediction, and unbiased results.

Data Sovereignty: Every data element that is contributed to the Endor.coin network can
be flagged as either “public” or “private” (the same data source may contain certain
columns marked public and others kept private). Public data sources are accessible to
every prediction engine, being a source of behavioral clusters for any future predictions.
In return, their providers are being compensated with EDR tokens when they are
selected by the protocol. Private data elements are still accessible by the various
prediction engines, albeit in an encrypted way. The clusters extracted from such sources
however can be selected as a source for predictions only when the user who requested
these predictions provides the key for the data. See more details in Section 5.4.

Accountability and Censorship Resistance: Using Blockchain, predictions are stored
indefinitely, and can be accessible by anyone who is interested in deducing the rep-
utation of the platform, the data that was used for it, or the prediction engine that
analyzed it. In addition, as the Endor.coin protocol contains an open-source predic-
tion code [33] that is in charge of selecting the behavioral clusters that are used for
the generation of each prediction (and the arbitration of the funds paid for it), it is
guaranteed to be free of bias, optimizing accuracy only. See more details in Section
4.2 and 4.1.

Prediction Efficiency for All: Ultimately, the Endor.coin protocol enables end-users to
obtain superior predictions at a low cost. This is based on the automatization of the
process (saving the need to employ expensive full time data scientists), and the ability
of social Physics to seamlessly ‘fuse’ together various types of behavioral data sources.
This means that even large commercial customers, such as Coca Cola, would be able
to immediately benefit from migrating to the Endor.coin network – by flagging their
data “fully private” they could be offered predictive insights that are based on the
fusion of the proprietary data, fused together with public data that was contributed
to the system, and pay an extremely lower fee that the alternative cost of obtaining
this data on their own, and analyzing it. This also offers a positive network effect –
lowering the cost as more users and data providers join the Endor.coin network (see
more details in Section 4.3).

9



Chapter 3

Democratizing the Prediction Science

3.1 Social Physics – a New Science from MIT

Social Physics is a revolutionary new science which uses big data analysis and the math-
ematical laws of biology to understand the behavior of human crowds, enabling Endor to
overcome traditional Machine Learning limitations. This new science originated at MIT
through research by Prof. Alex “Sandy” Pentland and Dr. Yaniv Altshuler. It was fur-
ther developed by Endor using proprietary technology, resulting in a powerful engine that is
able to explain and predict any sort of human behavior, even when the behavior is rapidly
changing and evolving.

Simply put, Social Physics is based on the premise that all event-data representing human
activity (e.g. phone call records, credit card purchases, taxi rides, web activity) is guaranteed
to contain a special set of human activity patterns that are embedded within that data.
These mathematical invariances, which are common to all human data-types, across all
demographics, can then serve as a filter for detecting emerging behavioral patterns before
they can be observed by any other technique.

Illustrating the Power of Social Physics: Imagine that the marketing department of
a large bank constantly calls customers who are potentially in need of a loan in the near
future. The revenues of the department are directly derived from As the direct-marketing
costs involved in this ongoing campaign are significant, it is crucial to contact the right
customers, at the right time: too late, and they will already might have taken a loan from
another source. Too soon - and the need has materialized yet.

For this, they consider two tools to predict who these customers are: A Machine Learning
model developed in-house by the banks data science team; and Endors engine. Here is a
simplified representation of what each tool recommended:

The group of customers that were detected by the Machine Learning model comprised of
customers who will respond positively to a marketing offer by the bank (e.g. True Positives),
as well as of customers who will not (e.g. False Positives). For example, lets assume that the
True Positives are 10% of the models results. Extensive experiments show that we can expect

10



the vast majority of those 10% to also be detected by the Endor Social Physics engine, with
two main differences: (a) many of the False Positives of the Machine Learning model will
not be reported by the Endor engine; (b) Endors results will contain many additional True
Positives, not detected by the traditional model. The result was a significant improvement
in the sales efforts, thanks to Endors the better precision / recall trade-off.

How? Detecting Temporal Patterns: Human reality is composed of many small tem-
porary events and changes. Social Physics incorporates the underlying dynamics of human
behavior and is therefore better equipped to uncover small groups in the population who are
likely to behave in a certain way due to recent changes in their social environments. Social
Physics is therefore uniquely capable of identifying dynamic signals in human behavior data:
This is because without the aid of Social Physics such signals lack any sort of statistical
significance, rendering them indistinguishable from noise for traditional Machine Learning
and Deep Learning methods.

Machine learning and Deep Learning vs. Social Physics – Which one is better
for which purpose? Solving a business query using data science and big data analytics
tools, both Machine Learning and Social Physics are viable options. The table below can
help identify the appropriate tool, based on its attributes.

Why Social Physics? Rooted back in the 70s the various mathematical and statistical
Machine Learning techniques were historically developed for ‘static problems’, such as image
processing and text recognition. Such problems are dominated by a relatively small number
of relatively stable ‘signals’. A trained text recognition model would achieve similar perfor-
mance when processing the handwritten text of a 2016 MIT student and when analyzing

11



Albert Einsteins personal letters. Similarly, neither Siri nor Googles speech recognition en-
gine would find it difficult to transcribed a high-quality recording of J.F.Ks famous “ich bin
ein Berliner” speech.

Human behavior, however, is a different story. Governed by a multitude of ‘dynamic
signals’ it is highly dynamic and highly ‘fractured’. A traditional Machine Learning model
trained to detect Millennials from credit-card purchases rapidly deteriorates in accuracy over
time, requiring constant maintenance by a skilled expert continuously incorporating new
semantics knowledge into it. As Millennials behavior is subject to frequent (and constantly
changing) trends, locating this in the data dictates not only a constant re-training of the
model, but also the frequent development of new features intended to detect these trends
(i.e. complex aggregative behavioral properties that are not part of the raw data). This can
only be done through the combined work of a semantics domain expert working side by side
with a data expert.

Following are the primary advantages of using Social Physics as a tool for behavioral
prediction in human data:

12



13



14



3.2 1st Phase : Endor.com – Automatic Prediction En-

gine for Enterprizes

As discussed in the previous sections, the state of Data Science today, as well as the available
Machine Learning technologies, dictate that the use of such capabilities was preserved for
deep pocketed tech giants. Companies who can afford a Six-digits annual salary for these
growing-in-rareness experts. However, even for those companies Data Science and Predic-
tive Analytics were far from being a commodity. On the contrary – a project aimed for
producing reliable predictions would typically block a team of 2-3 experts for 4-6 iterations
of a few weeks each, and after the first model is produces – would usually require a constant
maintenance. Industry standards therefore estimate the overall cost of an average prediction
project at approximately $1.5 Million. This means that:

• Most companies cannot afford to fund more than a handful of projects.

• In order to have a positive ROI projects today have to show huge margins, and focus
only on the most central business aspects.

• Companies constantly have to prioritize, aiming the super-expensive weapon of Data
Science only on the projects they think would result in (a) technological success + (b)
high business returns.

Endor, an MIT spin-off, was established around the novel science of Social Physics, in an
aim to solve this problem, by disrupting the way Data Science is being perceived today. Based
on 3 years of research at MIT, and an additional 3 years of development by an elite team of
researchers and engineers, Endor have developed the world’s first ever fully-automatic “Data
Science as a Service” engine, that allows companies to onboard any behavioral data they
possess, and after a quick integration (that typically requires a mere few hours) start asking
an unlimited amount of prediction questions regarding the future behavior of any ‘object’
(users, products, coupons, locations, etc.) contained in the data.

For an annual cost of less than $1 Million, companies were given the possibility to ask
dozens of predictive questions, getting quick results within minutes. This is not just a linear
improvement, but rather a paradigm change, as executing a ‘prediction project’ became as
easy as ‘Googling’, virtually rendering the prioritization of potential projects unnecessary,
as any decision could have now become prediction-oriented.

Example I – Coca Cola Joint Study: In a recent collaboration between Endor and
Coca-Cola [16] the ability of Social Physics to provide accurate predictions regarding a wide
variety of consumer behaviors was demonstrated. These included brand defection and loyalty,
adoption of new products, response to marketing campaigns and others. Using millions of
point-of-sale transactions as its raw input (representing a 3-months period), Endor’s Social
Physics engine detected nearly 20 million ‘correlated anomalies’, each representing a single
real-world behavioral group. Whereas the exact meaning of each group is unknown, the

15



groups are then used for ‘behavioral extrapolation’: given a sample-set (e.g. early-adopters
of a new product), the system uses the collection of behavioral groups to find lookalikes users
who are behaviorally similar to the members of the sample-set (e.g. users likely to experience
the new product soon). Using this methodology 15 different predictive questions were asked,
each yielding a prediction report on demand. Reports high accuracy was demonstrated using
out-of-sample data kept for validation.

“Social Physics is about behavioral analysis in big data, but it takes it to a
completely new level. We were very fortunate to find Endor and work with it.”

Dr. Alan Boehme, CTO, the Coca Cola Company

Example II – Automatic Analysis of Tweets: In a recent test 15 million Tweets meta-
data were provided to the Endor engine as raw-data for analysis. In addition, the customer
revealed the identity of 50 Twitter accounts known to be ISIS activists that were contained
in the input data, and tested Endors ability to detect an additional 74 accounts that were
hidden within the data. Endors engine completed the task on a single laptop in only 24
minutes (measured from the time the raw data was introduced into the system until the
final results were available), identifying 80 Twitter accounts as ‘look-a-likes’ to the provided
example, 45 of which (56%) turned out to be part of the list of the 74 hidden accounts.
Importantly, this provided an extremely low false alarm rate (35 False Positive results), so
that the customer could easily afford to have human experts investigate the identified targets.

“A revolutionary concept and a truly technological breakthrough. The results
they presented are unmatched by any competing tool.”

CIO of Israeli Intelligence Corps

16



The Need for an Ecosystem-Enabling Decentralized Protocol: Using Social Physics
and a strong team of professional, Endor’s engine has demonstrated how a large Fortune-500
company can pay significantly less, and get significantly more. However, this product was
made available mainly for large banks and retailers, and it still comes at a cost ranging from
$250K to $1.2M for an annual license. This is of course not a deal that is applicable to most
‘long tail’ businesses, not to mention individuals.

Therefore, a need for another solution became apparent. One that would allow anyone
to benefit from the new technology of Social Physics, and to do so at a reasonable price.
Such solution must therefore:

• Be self-reliant, comprised of the resources made available by its ad-hoc participants.

• Generate a strong network effect, incentivizing participants to continuously join it,
becoming better and cheaper as it grows.

• Preserve fairness, and be inherently unbias and trustless. This is requirement as unlike
other services, prediction is expected to have what can be called ‘an external effect’,
caused by the action of the person or organization relying on it.

With this concept in mind, the Endor team is proud to present the next step of its
revolution, the Endor.coin Protocol!

3.3 2nd Phase : Endor.coin Protocol – Data Science

for the Masses

Following the successful industry implementation of Social Physics inspired behavioral anal-
ysis by Endor, the Endor.coin Protocol created with the aim of bringing this ability to the
long-tail business, as well as professional individuals. Reinventing predictive analytics, the
Endor.coin Protocol democratizes Artificial Intelligence for behavioral prediction – enabling
the creation of an ecosystem that makes it accessible to all. Furthermore, the protocol’s
fully-decentralized nature guarantees trustless, censorship-resistance and accountability. For
the first time, behavioral predictions become available for all, for an affordable fee, in a
secured framework, free from potential manipulation of tech-giants who control data and
technology today.

3.3.1 Data Providers

The Endor.coin Protocol supports the integration of any behavioral, time-associated struc-
tured data. Data onboarding is done using a simple API call, allowing data owner to con-
tribute data while controlling which columns remain private and which become accessible
to the public analysis. Data defined by Endor.coin users to remain private is still auto-
matically integrated with public data-streams, producing high-quality predictive insights
harvested from the fusion of privacy and public data. Using the notion of Social Physics’

17



Knowledge Sphere (see Appendix D) data integration is automatic and friendly – requiring
no cleaning or data-preparation.

Data providers are required to pay EDR tokens for the analysis of their data, to the
providers of prediction engines. They are, in turn, being rewarded EDR tokens when insights
derived from their data are being used for predictions. This incentivizes the contribution
and maintenance of high-quality data streams.

3.3.2 Prediction Engines

The Endor.coin Protocol defines a ‘prediction language’ that is based on the projection of the
data to a feature space of partially overlapping behavioral clusters. The current Endor engine
would become the first prediction engine to be plugged in to that network, in order to make
it usable immediately after launch. However, Endor.coin would facilitate, assist and fund
the development of new prediction engines, aiming for the creation of an ecosystem that
is comprised of multiple types of engines, providing complementary capabilities, boosting
performance accuracy and increasing reliability. The growing number of prediction engines
would also provide guarantee for the unbias nature of the results, as predictions would be
created based on the most relevant clusters, automatically chosen by the Endor.coin open
source Protocol from the collection of clusters, extracted by the various prediction engines.

3.3.3 Pre-Defined Predictions and Request for Predictions (RFPs)

The Endor.coin Protocol will be launched with a large catalogue containing a variety of pre-
defined predictions. These predictions would be accessible for purchase using the platform’s
EDR token. Endor.coin would then expand the selection of predictions it caters by allowing
users to send Requests For Predictions (RFP) – suggesting new types of predictions, that
would be implemented and become available for purchase.

This method of gradually expanding the predictions supported by the platform would
utilize the wisdom of the crowd (as manifested by the requests of the protocol users) to
optimize the selection of newly supported predictions.

3.3.4 Private-Data Analysis and Self-Serve API

The later releases of the Endor.coin Protocol would include support in a complete ‘do-it-
yourself’ API for advanced users: tech-savvy users and professional Data Scientists would
be able to use a self-serve interface to easily onboard proprietary data and create on their
own new types of predictions. These predictions can be defined as private, or – be shared
with the public (rewarding the prediction develop with EDR tokens, if they become widely
used.

18



3.4 Roadmap

In todays general software market, our approach can be compared to offerings such as
platform-as-a-service or more recently blockchain-as-a-service. The Endor.coin tokens (or
EDR ) will be used to power transactions on the platform. EDR serve as a key or software
license, and more tokens can be used over time to increase performance and scale via a com-
munity of developers that will be enticed to expand the areas of applications. In addition,
a dashboard will be created for our administrators to distribute tokens, monitor usage and
purchase more tokens as necessary.

The next step in the Endor.coin evolution will be to expand our ecosystem organically
by an ever growing community of Catalysts. Everyone can provide RFPs (request for predic-
tion), paid in EDR , and the challenge of addressing the respective RFP can be undertaken
by any Catalyst who will be rewarded from the respective payments through a smart contract
for contributing to expand our range of predictions which we will embed as newly supported
queries by Endor.coin (further accessible to everyone, using EDR ).

In this second phase Endor.coin will become a modular App platform on which Cata-
lysts can expand the prediction domain through the power of Blockchain. To enable this
Endor.coin is endowed by an open ended stream of micropayments to authors of reusable
software components that can be perpetually combined and recombined to create an ever
expanding library of useful, highly customizable query Apps. Catalysts are entitled to a
Micro Use License for each component they add to Endor.coin platform. End users install
the Apps of their choice. The license to be paid is the sum of all the Micro Use Licenses of
the components used by that App. Through smart contracts Endor.coin is responsible for
charging end users and distributing the payments to the respective Catalysts involved.

With time Endor.coin will become the premier development platform for entrepreneurial
coders and enterprises looking to build data-rich web and mobile products on blockchain
thus democratizing Machine Learning which is so far accessible only to those highly skilled.
On top of that, existing blockchains that today do not support the volume of transactions
needed to efficiently execute data-analytics algorithms will be provided these capabilities,
through the help of an ever growing community of Catalysts who will build applications
using the engine continuously expanding the domain of queries.

Our vision is creating a fully automatic, trust-less decentralized predictions infrastruc-
ture, that will be fully transparent to end users. Incentives are paid in EDR tokens and
applications are essentially a set of plugins. A positive feedback loop is foreseen: the more
applications built, the more plugins are added to the system and more unique components
are ready to be reused. This contributes to mutually self-reinforcing network effects across
the Endor.coin ecosystem.

Ultimately we will deliver a Platinum platform for high end customers, where professional
experts and businesses can submit more complex fine grained RFPs. To deliver on such
complex queries we will expand our data access by rewarding providers with EDR as follows:
when a Catalyst requests data from an entity (timed limited) the respective entity proposes
a price (timed limited). If the Catalyst agrees to the payment the entity sends a link to the
data encrypted with the Catalyst public key (at which point the money is being disbursed

19



via a smart contract). Reviews can be published by both Catalysts and data providers (for
future reputation). EDR will be generated from the arbitrage between the data providers
payment and the funding received from the platinum application developer.

20



Chapter 4

Trustless, Censorship-Resistant and
Accountable

4.1 Accountability and Authenticity for Artificial In-

telligence

In his article “Why you want blockchain-based AI, even if you dont know it yet” [34], the
author Jeremy Epstein shares with his readers a conversation he and his young daughter had
with Amazon’s AI gadget Alexa. This conversation revolved around the topic of ‘Network
Neutrality’, during which Alexa shared with its listeners an abundance of relevant informa-
tion. Alas, given Epstein’s familiarity with the topic, he had surprisingly discovered the
information to, albeit accurate, potentially one-sided. The experience highlights some of the
risks of the AI-powered future into which we are hurtling at warp speed. It is also a reminder
that big companies, such as Amazon, have traditionally had big advantages when it comes
to big data and AI.

Quoting Epstein:

If the race is about gathering, storing, and analyzing as much data as possi-
ble, then who is in the pole position to win? Thats right, the FANGs in the
U.S. (Facebook, Apple, Netflix, Google), the BATs in China (Baidu, Alibaba,
Tencent), and the wealthy Fortune 1000 or so multinational corporations.

They are the only ones with the reach and capital to get more data, store it,
analyze it, and build AI models on top of it. Whats more, they are the only ones
who can offer starting salaries in the $300,000 to $500,000 range and top-tier
salaries that extend into to seven and eight digits. Your son or daughter may
not make it to the NBA or NFL, but become a top AI scientist and youre doing
great.

The net effect of all of this is that the rich become even richer and more powerful
and the barriers to innovation become even higher.

21



It is not only innovation that suffers, however. The closed nature of big-company
AI means society must put its trust in “black boxes.”

Providing “prediction authenticity” therefore relies on the availability of an infrastructure
that would provide all of the following:

• Accountability: providers of predictions can prove, in retrospect, that their predic-
tions were correct. Consumers of predictions can reliably deduce in retrospect the
efficiency (or lack of) of any prediction provider to produce accurate predictions of any
type. In other words, reputation should be impossible to manipulate.

• Authenticity: providers of predictions are who they say they are. Impersonation for
the cause of misleading prediction consumers should be impossible.

• No Bias: there should be fair competition among prediction providers, that would
utilize market forces to incentivize accuracy rather than implicit gain achieved through
biased predictions.

• Accuracy: predictions should be accurate enough, generally speaking, to provide
monetary inflow to the network, that would close the positive feedback loop with
respect to the previous three requirements.

The Endor.coin protocol provides an infrastructure that would suffice all of the above
requirements:

• Endor.coin Protocol Accountability and Authenticity: using blockchain, pre-
dictions are stored indefinitely, and are accessible by everyone. When time-sensitive
predictions are only sent to consumers, they are also stored in an encrypted-yet-public
version, with a key that is released after a certain period of time.

• Endor.coin Protocol Zero Bias: the protocol selects for each prediction the most
relevant behavioral clusters (regardless of the analytics engine, or data source). There-
fore, as the protocol is a separated entity, uncontrolled by data providers, as well as
analytics engines providers, bias is inherently prevented.

• Accuracy: Endor.coin is based on MIT’s Social Physics technology, extensively
proven by the industry to consistently provide accurate predictions for a large vari-
ety of use-cases.

4.2 Censorship-Resistance through a Decentralized Pro-

tocol

Censorship is a tricky business. When exists in its explicit manifestation (such as in countries
like North Korea) it is easy to detect, and therefore – to bypass, through the use of “low

22



level” technological solutions such as IP-proxies, and so on. Implicit censorship on the other
hand is a different issue altogether. It is known that companies such as Google and Facebook
block certain types of search queries. Many times this is done for the right moral or legal
reasons, but – how can we be assured that this is always the case? Can we “force” Google to
provide a relevant webpage, if it had decided that we “should not have an easy access to it”?
Can we “force” Alexa to comply with some of our requests it would deem inappropriate?
The answer is of simple – no.

Prediction in this context is very similar to Search, in the sense that it is subject to
censorship by the operating entity, or the regulatory agencies they are subject to. If we look
at how AI and predictive analytics work, they have three essential layers, with respect to
potential censorship:

• Data Repository: guaranteeing the integrity, completeness and security of the data
(are the inputs accurate and reliable, and can they be manipulated or stolen?)

• The Algorithm/Machine Learning Engine: making sure predictions are not in-
spected by a centralized authority, and that all prediction requests are being executed
fairly, with quality-of-service considerations unrelated to topic of prediction.

• Queries Interface: reliably representing the output of the prediction query, effec-
tively capturing new data, and having any limitations on supported predictions being
unrelated to topic of prediction.

If one is going to trust one’s decision-making to a centralized prediction source, one
implicitly assumes with absolute confidence that all the above requirements are met. In a
centralized, closed model prediction scheme, when one is asked to trust in each layer without
knowing what is going on behind the curtains, such confidence is difficult to justify (if not
plain gullibility).

The Endor.coin protocol inherently provides these assertions by allowing any prediction
to be executed, in a fully decentralized, trustless way. Once data was contributed and
onboarded to the network, any relevant prediction can be executed, having the Endor.coin
protocol optimizing it automatically.

4.3 Network Effects

The full-decentralization of the Endor.coin protocol results among others in several network
effects, that increase its merits as its usage widens. Following are the main expected thrusts
that will be triggered by an increased adoption of the protocol:

Adding users decreases the cost for each of them: Unlike large commercial cus-
tomers, individuals and small businesses require predictions that are based mainly on public
datasets. The Endor.coin protocol enables such datasets to be analyzed once – generating a
collection of behavioral clusters known as “Knowledge Sphere”. This data structure can then

23



support all of the prediction use-cases that are based on this data source, requiring end-users
to pay a small fee that encapsulates their “personalization factor” – the delta between the
extraction of the behavioral clusters, and the specific use-case they are interested in.

In other words, for a given dataset (i.e. the Bitcoin Blockchain), and for a specific
use-case, the cost is calculated as follows:

• Knowledge Sphere Calculation: Consists of roughly 99% of the overall cost, and is
being calculated once. Resources required for this task are amortized among all users
of this data-steam.

• Prediction Personalization Factor: Consists of roughly 1% of the overall cost, and
gets calculated (and paid for) for every users. In addition, for similar predictions made
by different users, the Endor.coin protocol automatically reuses insights derived by
the prediction that is calculated first, during the calculation of the later ones, further
improving prediction accuracy for the same cost.

This means that the governing cost equation of a network of N end-users would be:

0.99× C
N

+ 0.01× C

for C denoting the cost of the same prediction by a single commercial player. This ul-
timately means that as the number of users N increases, the accuracy of the predictions
increases, while its cost aspires to approximately 1% of the cost of the same system, used by
a commercial large customer.

Ultimately, the more people asking questions – better the results they receive, and for a
lower cost, as the vast majority of the cost is divided across all active users.

Adding data-providers increases accuracy: Per the definitions of the Endor.coin pro-
tocol, data providers are required to fund the execution of their data using EDR tokens, for
creating the “Knowledge Sphere” that is based on the behavioral clusters that are extracted
from the data. This initial execution cost is then being repaid to the data-providers in part,
equally, or with significant interest, by the end-users – all according to the quality of the data,
and the contribution of it to the various prediction queries. This funds arbitration is being
taken care of by the Endor.coin protocol, that can access the entire collection of behavioral
clusters detected by the various analytic engine, on the various datasets, and select the top
clusters of highest-relevance. Execution tokens are being then delivered to the providers of
these datasets, pro-rata, with respect to their contribution to the final prediction.

The result of this mechanism is that owners of high-quality data are incentivized to con-
tinue supporting their data sources (and even further increase their quality and availability),
while providers of data of poor quality are being rooted out, simply because their costs are
not being repaid. This economy utilizes the market forces to automatically guarantee that
the available data-sources to the prediction protocol remain of the highest possible fit. New
data-providers can therefore only increase overall prediction accuracy, without increasing
their overall costs.

24



Adding prediction-engines increases prediction efficiency: The first phase of the
Endor.coin project relies on the Endor prediction engine to act as the first provider of be-
havioral clusters extraction. However, in time additional prediction engines are expected
to support the Endor.coin protocol. The introduction of such new prediction engines is
expected to have a significant positive effect on both data-providers as well as end-users:
engines that utilize different technologies are expected to produce different types of clusters
from the same data source. This means that as the variety of technologies used for predic-
tions engines that support the Endor.coin protocol increases, so will the variety of clusters
which will become available for the protocol to select from, when new predictions are being
requested.

This is expected to have three main effects:

• Improved Accuracy, and Increased Support for New Predictions: As new
types of clusters become available, the Endor.coin protocol will be able to better select
clusters to be used for the generation of each requested prediction. For existing predic-
tions this would result in an increased accuracy (due to the availability of “orthogonal
insights”, derived from the use of a clustering technology). However, this would also
imply that new predictions, that until that point were not supported by the system
due to suboptimal accuracy, would now become cost-effective to execute, increasing the
variety of predictions supported by the system, and subsequently further boosting the
overall accuracy, due to the sharing of insights among predictions executed at adjacent
times.

• Reduced Cost per Prediction: The increase in the amount of predictions supported
by the system would also result in an increase in the number of end-users who pay for
predictions – immediately increasing the overall pool of available EDR tokens paid for
the initial data analysis, and subsequently increasing the amount of funds received by
the data-providers, while decreasing the cost-per-prediction for the end-users.

• Economic Sustainability of New Data Sources: Finally, the availability of new
clustering technologies may also render the contribution of certain types of data sources
economically feasible, in cases where these new technologies are better compatible
for the analysis of such data sources then the available ones. In such cases, such
data sources would suddenly become a viable source of information for the Endor.coin
protocol, repaying the cost of their initial integration to the platform, and subsequently
also increasing the amount of available data sources, with the benefits that are derived
from it.

25



Chapter 5

Token Implementation

5.1 Blockchain Structure

Data Storage: Blockchains are not general-purpose databases. Endor.coin has a decen-
tralized off-chain distributed hash-table storage that is accessible through the blockchain,
which stores references to the data but not the data itself. Private data is encrypted (using
AES-256 – Amazon S3 server-side encryption) on the client-side prior to transfer and stor-
age, having the access-control protocols are programmed into the blockchain. Endor.coin is
designed to connect to an existing blockchains as well as to private and public datasets in an
off-chain network (stored in any form of database, central storage, etc., that can be exported
into a structured format that is eventually uploaded to Endor.coin AWS infrastructure). In
future releases the Data Layer will be opened for external players which will be able to sell
data for EDR tokens. The data will be certified and uploaded to the Endor.coin off chain
infrastructure, where it will be available to the owners of the data, and if marked ‘public’ –
for any customers for which the prediction algorithm deem it as relevant. The pricing will
be determined by the data provider, and adjusted automatically by the Endor.coin protocol
dynamically, according to the demand for this prediction, incentivizing users to share the
cost for the same data source.

Consuming Predictions: The blockchain cannot handle heavy complex transactions.
The same off-chain computational network is used to run heavy computations (required
by the various predictive engines). Once results are available, they are being broadcasted
throughout the public blockchain for end users use (authenticated using the key of the
user who requested the prediction). In parallel, the same results are encrypted using a
different,. temporary key, and broadcasted publicly. This key is released after a pre-defined
time, allowing results to be authenticated later on, even by users who did not require them
originally. This mechanism can be adjusted in a way that creates multiple such temporary
keys, having varying cost that depends on the ‘freshness’ of the prediction.

26



Processing Data: A code is executed both on the blockchain (public predictions, Public
RFP orders, Private RFP orders) and on Endor.coin infrastructure based on using propri-
etary hyper-elastic computational layer running on AWS. As processing data and extracting
behavioral clusters requires a complex and expensive execution environment, later releases of
the Endor.coin protocol would open the execution layer of the protocol as well. This would
enable a new type of stakeholders to join the Endor.coin ecosystem – ones that specialize in
intensive execution. The larger the data to be analyzed and the heavier (computation-wise)
the clusters-extraction algorithm – the higher the price in EDR tokens to run it will be.

Payment: a user can pay for every prediction request an amount of EDR depending on
the prediction complexity, dynamically defined by the Endor.coin protocol, per the available
resources and the demand at the time of request, and the number of users asking similar
questions. Pre-defined queries are expected to remain at a relatively steady price, whereas
RFPs (Request for Prediction) would start with a relatively high cost (as by definition
they start with a small number of users), gradually decreasing as a community of users for
which it is relevant, is formed. The larger this community the less everyone eventually pays,
incentivizing the creators of new prediction queries to ‘spread the word’ among people in
their network.

5.2 Smart Contracts

The Endor.coin protocol provides several basic primitives for its end users:

• GetPrediction(prediction def) – to be implemented at Phase 1

• PutPredictionReq(prediction def) – to be implemented at Phase 2

• PutData(data def) – to be implemented at Phase 2

• RunCustomPrediction(data alg prediction def,price) – to be implemented at Phase 3
These primitives allow customers to retrieve predictions available at the Endor.coin plat-

form for a dynamic cost and later on upload data and sell it to be used for prediction purposes.
While the primitives cover the default use cases for the Endor.coin protocol, future releases
would enable for more complex operations to be designed on top of Get and Put by sup-
porting a deployment of smart contracts. As the protocol progresses towards private data
and custom private predictions, it will enable additional layers of security and complexity
on top of the basic smart contract that will be introduced as the first public predictions
version. Smart Contracts would enable Endor.coin users to write stateful programs that
can spend tokens, request predictions and be used for retrieval of data in the markets, as
well as validate data quality proofs. Users will be able interact with the smart contracts
by sending transactions to the ledger that would trigger function calls in the contract. The
Smart Contract system will be extended to support Endor.coin specific operations (proof
verification), supporting contracts specific to data upload (which will be used for public or
private use at later on stages), as well as more generic smart contracts.

27



getPrediction(prediction def): Allowing users to retrieve predictions stored on the En-
dor.coin network by paying EDR tokens. Clients initiate the Get protocol by submitting
a bid order to the Retrieval predictions Market order book (by gossiping their order to the
network). When a matching ask order from prediction provider, the client receives private
temporal link to the prediction. When received, both parties sign a deal order and submit
it to the blockchain to confirm that the exchange succeeded. The prediction consumer will
pay the equivalent price which was attached to the prediction that he asked to get. The
predictions will be accessible for download from the Endor.coin webportal.

putPredictionReq(prediction def): Allowing users to suggest additional prediction types
by submitting it to the Endor.coin platform. The interface will be accessible in the En-
dor.coin webportal. The suggested predictions will be visible to all Endor.coin users and
they will be able to rank them. The most highly ranked predictions will be added to the
continuously growing predictions catalog. Prediction requests will be identified with a public
address of the prediction issuer, once the prediction is added to the catalog, the issuer of the
prediction will be compensated each time the prediction is consumed by other users.

putData(data def): Used for data providers and partners. Data providers are rewarded
by EDR tokens whenever their data is being used for predictions, having the pricing auto-
matically calculated by the Endor.coin protocol prediction algorithm. This variant of the
function will be accessible in the second phase of the Endor.coin project. During the third
release of the protocol an API for a ‘do-it-yourself’ mode, allowing customers to upload their
proprietary data streams, marking them as ‘private’, and having them fused together with
the public data streams that are accessible on the platform. Client initiates the Put protocol
by submitting a bid order to the Storage Market order book (by submitting their order to
the blockchain), waiting for a matching Ask order to be placed from data validator (e.g.
the prediction engines providers). Clients are required to fund the extraction of behavioral
clusters by the prediction engines, but are then able to decide on the price they want to get
for each usage by the end users.

runCustomPrediction(data alg prediction def, price): This method would be im-
plemented at the third release of the protocol, supporting the creation of prediction that
are not included in the Endor.coin catalogue, on a ‘do-it-yourself’ mode. Input contains
the description of the desired behavior, by the way of example, or description of a logic
that refers to publicly or proprietary data. Maximal price is defined, as the request can be
handled by any available engine. The fees will be arbitrated between the different players
that contributed to the prediction (e.g. the prediction engine and data providers).

5.3 Catalysts – Application Developers

To increase the range of questions that Endor.coin can address, the domain is infinite and
limited only by those who want to build applications using the Social Physics engine, the

28



community of “catalysts”. Fueled by them, Endor.coin will become a modular App platform
on which Catalysts can expand the prediction domain through the power of Blockchain. To
increase the range of questions that Endor.coin can address, the domain is infinite and
limited only by those who want to build applications using the engine, the community
of “catalysts” Endor.coin will become a modular App platform on which Catalysts can
expand the prediction domain through the power of Blockchain. Endor.coin enables an
open ended stream of micropayments to authors of reusable software components that can
be perpetually combined and recombined to create an ever expanding library of useful, highly
customizable Apps. Catalysts are entitled to a Micro Use License for each component they
add to the Framework. End users install the apps of their choice. The license to be paid is
the sum of all the Micro Use Licenses of the components used by that app. Through smart
contracts Endor.coin is responsible for charging end users and distributing the payments to
the respective Catalysts involved. Everything is automated and transparent to end users.
Incentives are paid in tokens. Applications are essentially a set of plugins, a positive feedback
loop is foreseen: the more applications built, the more plugins are added to the system and
more unique components are ready to be reused. This contributes to mutually self-reinforcing
network effects across the Endor.coin ecosystem.

5.4 Data Sovereignty

In a world which consumes our privacy for the benefit of greedy corporations, Endor.coin is
committed to changing the game and return the wealth back to those who provide access to
their data. A simple sign-up on Endor.coin starts the process which can unfold according to
various scenarios enabled by the EDR tokens. Members can receive individual insights based
on their own data or team up as a group to aggregate a larger pool of data working together
with a small business to get more personalized services, such as for example zooming in on
the right product at the right time for the best price.

Based on the fundamental belief that people have the right to own, control and benefit
from the data they generate Endor.coin opens the power of insights currently available only
for those with deep pockets, to the so far disadvantaged long tale of small businesses which
can now aggregates Endor.coin Members digital data for their own use, with the data owners’
permission, to provide high value data analysis that brings back to its members both highly
efficient services resulted from that analysis, as well as monetary gain – when the data is
utilized in other ways. This Personal Data Independence based approach, championed by
Endor.coin unlocks tremendous value for both companies and people, without loss of value
or trust for either. Endor.coin supports businesses by coopting membership and brand
participation at a negative acquisition cost as companies and other organizations sign up
to eliminate data liability, access superior data about their customers, and grow a better
customer relationship.

As membership will continue to increase, Endor.coin will also provide a suite of tools and
services that will help members gain insights and understanding regarding many aspects of
their lives from their own data. With the mission “Data of the People, by the People for the

29



People” Endor.coin gives back to the individuals their inherent right to capitalize on their
lives which have been so far violated by Facebook, Google, Uber and many other data-based
conglomerates, thriving on the “platform economy”. Through a multi-faceted approach
that integrates proven enterprise expertise, forward legal thinking, technical know-how and
value creation for consumers Endor.coin offers a unique business model which resolves the
“data piracy” problem by creating a consensual data relationship which enables companies
to generate good will with their customers by rewarding members directly for access to their
data insights.

To ensure the integrity of the process we are creating Endor.coin Trust, an independently
managed entity that works in a synergistic relationship with Endor.coin to maximize the
benefits, value creation and data security for members. As people sign up to Endor.coin
their data is overseen by the trust and they automatically become trust members. The
Endor.coin Trust represents its members in many ways to ensure and maximize the safety,
security, privacy and value of their data. The Trust protects members data by setting and
enforcing standards of personal data control, value realization and privacy.

30



Chapter 6

Technological Advantages and
Differentiation

6.1 A Scientific Revolution from the MIT Furnaces

Powered by MIT’s novel Social Physics technology [1], Endor.coin utilizes the world’s most
advanced behavioral prediction technology. This scientific breakthrough that had started at
the MIT Media Lab at the beginning of this decade revolutionized the field of behavioral
data analysis, and was in charge for technological accomplishments such as winning the
prestigious DARPA Network Challenge [21], boosting the returns of a community of retail
investors [9,13] and successfully forecasting the existence of efficient unknown cyber-attacks
[35]. This technology was developed by a team of academic and industry experts, that
have collectively published hundreds of scientific papers, dozens of papers, and several books
dedicated to these subjects.

And now, this revolution reaches the public, utilizing blockchain technology in order
to provide professional and laymen alike access to capabilities that hitherto were the sole
privilege of large retailers, banks and tech giants. A detailed discussion about Social Physics
appears in Section 3.1 and Appendix A.

6.2 Real Product. Proven Technology

As previously presented in Section 3.2, Endor is an MIT spinoff [14] that took upon itself to
carry out the implementation of Social Physics as a product, designed to offer large banks
and retailers a SaaS solution that would open their predictions bottleneck. The company is
backed by leading investors [15], working with Fortune-500 companies such as Coca Cola [16],
MasterCard [17], Walmart and others, successfully demonstrating its ability to automatically
generate accurate predictions for a variety of use-cases.

The value of the technology for blockchain and cryptocurrency analysis was demonstrated
in [27] and others. Endor’s product has been featured at leading venues, such as Money-2020
or 72Finnovate 2017 [18]. Endor is a Gartner Cool Vendor [19], and was recognized by the

31



World Economic Forum as a “Technological Pioneer” [20].

6.3 Usability, Value to Users and Value for Token Hold-

ers

A key uniqueness of Endor.coin is that the EDR Token will be usable at day 1 of the token
launch – offering token holders complete access to the pre-defined predictions. In addition,
a group of beta users will be selected shortly after token launch, and will be given the
opportunity to request predictions in addition to the pre-defined ones.

Furthermore, token owners who believe they might require affordable access to predictions
in the future would be incentivized to buy and hold EDR tokens, as expected increased usage
will result in increased cost for prediction, driving token value up, rewarding prediction users,
who purchased EDR tokens early on.

32



Chapter 7

Team

7.1 Key Team Members

Dr. Yaniv Altshuler
Dr. Altshuler is the CEO of Endor, and a research affiliate at the MIT Media Lab, were
together with Prof. Pentland he developed “Social Physics”, a new science that models
crowds behavior. At MIT, the technology was used to win the prestigious DARPA
Challenge [21], help the Government of Singapore improve its ability to predict traffic
jams, assist a community of thousands of financial investors to improve their financial
returns [13]. At Endor, the technology was used to accurately predict the behavior of
crowds in a large variety of use-cases, efficiently catering large banks and retailers [14].

Prior to his position at MIT and the incorporation of Endor, Altshuler was a researcher
at IBM, developing a novel optimization technique used to boost the performance of
supercomputers. Active in Blockchain research since 2011, Dr. Altshuler has authored
over 60 scientific papers and filed 15 patents. Altshuler’s works have been featured in
popular venues such as the Financial Times [10], Harvard Business Review [11] and
others. His recent published books are ‘Security and Privacy in Social Networks ’ [26]
and ‘Swarms and Network Intelligence in Search’ [25].

Prof. Alex “Sandy” Pentland (Chairman of the Scientific Advisory Board)
Director of the MIT Media Lab Entrepreneurship Program, as well as the MIT Con-
nection Science and Human Dynamics labs. Prof. Pentland is one of the world’s
most-cited scientists [23], and was recently declared by Forbes as the “7 most power-
ful data scientists in the world” along with Google founders and the Chief Technical
Officer of the United States [24]. He has received numerous awards and prizes such as
the McKinsey Award from Harvard Business Review [22], the 40th Anniversary of the
Internet from DARPA [21], and the Brandeis Award for his work in privacy.

He is a founding member of advisory boards for Google, AT&T, Nissan, and the UN
Secretary General, a serial entrepreneur who has co-founded more than a dozen com-
panies, as well as social enterprises such as the Data Transparency Lab, the Harvard-

33



ODI-MIT DataPop Alliance and the Institute for Data Driven Design. He is a member
of the U.S. National Academy of Engineering and a leader within the World Economic
Forum. His most recent books are ‘Social Physics ’, and ‘Honest Signals ’.

Stav Grinshpon
Mr. Grinshpon is a veteran tech-industry expert, with 18 years experience of product
and management in companies such as SAP and AT&T. Grinshpon is a world expert in
cyber defense, serving 8 years as a technological leader at the Israeli 8200 technological
unit. Grinshpon is the author of 3 patents focusing on data analytics, and headed the
R&D activities at Endor.

Inbal Tirosh
Ms. Tirosh is an experienced global product and strategy leader. After spearheading
new business launches at Hewlett Packard Better Place, she now leads Endors strategy
and operations. She is a world-renowned expert at partnering with Fortune-500 enter-
prises such as Microsoft, Mastercard, and Walmart in unlocking value and innovation
by leveraging cutting edge technology. Ms. Tirosh holds a joint MBA from Columbia
University and Ben Gurion University, and a BA from Ben Gurion University, both
with honors.

David Shrier
David is a seasoned growth catalyst with expertise in data analytics, fintech, digital
identity and cybersecurity. He is a globally recognized authority on financial innova-
tion, and leads the University of Oxford’s online programmes Oxford Fintech and Ox-
ford Blockchain, as well as MITs fintech class, all of which he created. He is a frequent
speaker at the World Economic Forum Davos, author for Forbes on blockchain [36],
and has published multiple books on fintech, blockchain and cybersecurity [27,29,32].

David is on the advisory board of WorldQuant University, a program offering a totally-
free, accredited, online Masters degree in financial engineering, and recently advised
the European Commission on commercializing innovation with a focus on digital tech-
nology. He is a member of the FinTech Industry Committee for FINRA, the securities
industry’s self-regulatory body.

Prof. Mihaela Ulieru
Expert in Computational Intelligence, is a Blockchain Champion at the World Eco-
nomic Forum [37]. Her research in distributed intelligent systems created a strong
foundation for governance on Blockchain as an institutional technology after it revo-
lutionized manufacturing, logistics and homeland security. Prof. Ulieru, a California
Berkeley Alumna, is a member of the World Economic Forum’s Global Agenda Council,
the Science and Engineering Research Council of Singapore and the Canadian Science
Technology and Innovation Council.

Dr. Goren Gordon
Dr. Gordon is the head of the Curiosity Lab at the Tel Aviv University, where he de-

34



velops state-of-the-art models of computational curiosity. Gordon, a leading expert in
Deep Learning and Neural Networks Optimization, holds a PhD in Quantum Physics,
and another PhD in Neurobiology. Dr. Gordon had utilized this experience, together
with his additional degree in Medical Sciences, during his work at MIT Media Lab
with Prof. Cynthia Breazeal, where he studied how curious robots interact with cu-
rious children. Gordon is also interested in scientific education, a cause for which he
developed “Quantum Computer Games” that teach Quantum Physics to children via
play. Gordon is also a popular lecturer on the topics of quantum physics, the brain
and inter-disciplinary thinking.

Dr. Arie Matsliah
Dr. Matsliah is a world expert in the theory of graphs analysis, and spent 16 years
working for industry giants such as Google, IBM, Inteland Lyft as well as being the
Chief Architect at TripActions, a successful Menlo-Park based Start-Up. Dr. Mat-
sliah has also published over 40 scientific papers, focused on fundamental research in
algorithms, complexity, and quantum computing.

Shahar Somin-Gavrielov
Ms. Somin-Gavrielov is an expert in Statistical Learning Theory, holding a Masters
Degree (with honors) from the Hebrew University. Somin-Gavrielov is a seasoned
researcher, combining deep theoretic realization of data science with hands-on industry
experience. In her past, she was a decorated analyst at the Israeli 8200 intelligence
unit.

Edo Eisenberg
Financial Risk Management professional, previously managed retail credit portfolios at
Morgan Stanley and Barclays. Additional 9 years experience at NICE, designing and
implementing fraud prevention solutions deployed in 7 of top 10 U.S. banks. Alumnus
of the Duke MBA Program.

Lior Regev
Mr. Regev is a seasoned software engineer with a great passion for technology. Regev
is an ex-intelligence tech-leader, with vast experience in distributed systems, cloud
architectures and SaaS products.

Liat Yitzhaki
Ms. Yitzhaki is an expert on public law litigation. She holds an MA in Law and Ethics
and an LLB with honours from the University of London. In the past Yitzhaki worked
for McKinsey & Co., as well as a senior legal adviser to the British Government.

7.2 Advisors

Prof. Alexander Lipton
Professor Lipton served as the Managing Director of the Global Quantitative Solutions

35



for Bank of America, as well as the Managing Director of Global Quantitative and
Credit Analytics groups at Merrill Lynch. Prior to this, Prof. Lipton was the head
of Quantitative Research at Citadel, and Head of Equity Derivatives at Credit Suisse.
Lipton is currently a professor of Financial Engineering at EPFL as well as a Fellow at
MIT Connection Science Center, and recent author of the Scientific American article
titled “How technology could help fix our broken financial system” [38].

Ron Gross
Ron Gross is the Founder and a Board Member at the Israeli Bitcoin Association. He
has been active in the Bitcoin and Blockchain worlds since 2011, and was the Executive
Director of Mastercoin (now Omni) – the world’s first ICO. Gross in an ex-Googler,
and has also served as the Chief Architect at Commerce Science.

Dr. Daniel Tunkelang
A World expert in Data Science, Tunkelang, an ex-Googler as well as ex-IBM-er, is an
advisor to tech giants such as Apple, Salesforce Etsy, Yelp and Pinterest. Previously,
Tunkelang had served as the Director of Engineering in Search at LinkedIn as well as
the latter’s Data Scientist in Residence, as well as being the Chief Scientist at Endeca
(acquired by Oracle). Tunkelang holds an Masters Degree from MIT, and a PhD from
Carnegie Mellon University.

Prof. Michael Bronstein
Serial entrepreneur and a leading researcher, Prof. Bronstein is a Research Fellow
at Harvard University, and a Professor of Informatics at the University of Lugano.
Bronstein has authored over 100 publications in leading journals and conferences, over
20 patents, the research monograph “Numerical Geometry of Non-Rigid Shapes”, and
edited 4 books. Bronstein is one of a few researchers who received 3 European Research
Council (ERC) grants, and was also awarded the Google Faculty Research award,
the Radcliffe fellowship from Harvard University, and the Rudolf Diesel industrial
fellowship. He was selected by the World Economic Forum as one of the world’s 40
leading researchers under the age of forty.

Prof. Bronstein is actively involved in industrial applications. He co-founded and
served as Vice President of technology in Novafora (licensed to Turner Broadcast) and
was a co-founder and one of the main developers of the 3D sensing technology company
Invision (acquired by Intel, where Bronstein currently serves as a Research Scientist for
Perceptual Computing). Bronstein is a co-founder and technical advisor of Videocites.

Dr. Wei Pan
Dr. Pan is an MIT Alumni, ex-Googler, and a world expert on big data analytics,
machine learning and complex systems. He is the inventor of the analytic approach
known as “Reality Hedging”, that tries to understand financial markets dynamics and
macro economics through the understanding and models of social systems, and big
data measurements of people and crowds. Currently, Dr. Pan is a Co-founder and
Chief Scientist at Thasos Group, a New York based start-up. Pan previously worked at

36



Fidelity Investments under the Chief Economist where he focused on systemic research
and the Flash Crash.

Thomas Hardjono
Hardjono is a security expert, specializing in decentralized identity, blockchains and
smart contracts. Hardjono was the Executive Director of VeriSign and the MIT Ker-
beros Consortium, and have published 5 books dedicated to computer security and
cryptography [39–43].

37



Appendix A – Social Physics
Explained

38



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
1     

 

 

1. HOW SOCIAL PHYSICS WORKS? 

 

 

Clarification: This document intends to provide a comprehensive view of Social Physics, the high level principles behind it, as well 

as its technical implementation by Endor. However, certain technical details regarding the specific mathematical formulation of 

the Social Physics Laws that are used by Endor, as well as the specific implementation of the mechanisms used in order to detect 

violations of these Laws, were intentionally omitted, due to IP considerations. 

 

In the Information Age, companies gather data of all types and from numerous sources 

about their businesses operations. Data encompasses images and videos, text and tweets, 

transactions and usage logs. However, the majority of data originates from a single 

underlying source: People.  

Thus, for example, tweets and blog posts are written by humans for humans; purchase 

transactions and phone call information convey human desires for things and other 

people; usage and app logs report on how people interact with computers and mobile 

devices. 

Data derived from human behavior is “messy”: it is dynamic, complex and extremely 

versatile. Humans’ behavior, as recorded in such digital data channels, changes drastically 

over time, is influenced by underlying complex social networks, and is conveyed in highly 

multimodal data streams. These characteristics pose significant challenges for companies 

that wish to analyze, understand, and predict their customer behavior in order to improve 

their business operations.  

In recent years, data scientists have started to employ “heavy-weight” statistical methods 

and Machine Learning algorithms to try and cope with this complexity. These powerful 

tools, including the new “Deep Learning” techniques, collect data and analyze its 

attributes in order to be able to classify behavioral patterns, detect anomalies, and predict 

future trends. However, such tools – historically developed for “static problems” such as 

image processing and text recognition – cannot easily cope with human behavior data: 

learning dynamic, complex, and versatile data streams is extremely hard and sometimes 

nearly impossible. 

 

 

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
2     

 

 

 

 

 

 

Endor’s Social Physics engine works in a completely different way. Instead of deriving patterns 

from input data itself, it is based on the discovery that all human behavioral data is guaranteed 

to contain within it a set of common “social behavioral laws” - mathematical relationships that 

emerge whenever a large enough number of people operate in the same space. These laws 

govern the way various statistical properties of crowd behavior evolve over time, regardless 

of the type of data, the demographics of the users who created it, or the data size. Endor has 

integrated these laws into its data analytics engine, which efficiently extracts the underlying 

social attributes of all people contained in the raw data being provided as input (e.g. phone 

calls, taxi rides, financial investments, etc.).  

 

 

 

 

 

 

 

 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
3     

 

 

2.1. WHY SOCIAL PHYSICS IS NEEDED? 

 

Abstractly, Learning Problems, or the ability to classify objects or to produce predictions for 

future events, requires data to be analyzed, and an algorithm that analyzes it. The quantity of 

the required data depends on several factors: 

 

1. Internal Complexity of the Problem – problems come in different shapes and sizes, and 

some are undoubtedly harder than others. The “hardness” or “complexity” of a problem 

refers to the minimal “strength” a learning algorithm must possess in order to successfully 

learn the problem: if a learning algorithm is not strong enough, it will simply not be able 

to learn an instance of the problem correctly. For example, it can easily be shown that a 

“Perceptron” (i.e. the simplest Neural Network, comprised of a single neuron) can never 

learn the “XOR” function (namely, the Boolean “Exclusive-Or” function). The reason is 

embedded in the way a Perceptron works (which can be imagined as linearly dividing the 

input space) whereas the XOR function is simply “too complicated” to be represented this 

way: 

 

 
An example of the XOR function appears in the right chart (Source: Wikipedia) 

 

 

The learning-complexity of the underlying model of a problem is often referred to as the 

Vapnik–Chervonenkis Dimension (or VC Dimension) of the problem. The higher the 

learning-complexity of a problem is, the stronger a prediction algorithm needs to be in 

order to be able to learn it successfully, the more data such algorithm requires in order to 

properly model it. For example, a function y = f(x) that has a linear complexity (meaning, 

it can be well-modeled using a polynomial of rank 1) requires, by definition, less sampling 

points than a function that requires a polynomial of a higher degree in order to be 

accurately modeled. 

 

 

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
4     

 

 

 

 

Learning Efficiency of the Algorithm  

There are many learning algorithms, each requiring different quantities of training data 

and expert domain knowledge to properly ascertain the model parameters. For example, 

simple regression requires large amounts of data and many problem-specific features to 

work well, whereas deep learning, while requiring vast amounts of data, can 

automatically learn the domain features. For our discussion we can therefore quantify the 

overall efficiency of the learning process, that is – how much data would be required in 

order to learn the underlying model, and how much does the algorithm learn by itself the 

proper representation: 

 

𝜼𝑨 =
𝑹𝑨(𝑴)

𝑫𝑨(𝑴)
 

 

M = bits of model, is the number of bits required to formulate or model the data, e.g. 

number of parameters a perfect description of the model would require. In this sense, M 

is akin to the Kolmogorov Complexity of the problem – a known theoretical measure of 

the data complexity, referring to the shortest Turing Machine that can generate a given 

data. The model is determined by the problem and cannot be changed, i.e. if it’s a simple 

problem then the model has a few bits. 

 

RA(M) = bits of model-specific representation, is the number of bits learned by the 

algorithm A, to represent the underlying model M. This number represents the automatic 

feature detection of the algorithm and is inversely proportional to the number of domain-

specific knowledge experts must manually program into the algorithm. 

 

DA(M) = bits of data, is the number of bits required by the algorithm A, to learn model 

M.  

 

ηA = learning efficiency of the algorithm A, that is, the ratio between the automatic 

representation learning of the algorithm and the amount of data required to preserve its 

learn quality. A high efficiency algorithm can learn the proper representation with small 

amounts of data, whereas a low efficiency algorithm requires manually crafting features 

and vast amounts of data to tune them. 

For example, given a problem class M, some algorithms would require more data than 
others, to preserve the learning quality: 

- Logistic Regression algorithms usually require manual coding of features by experts 
and large amounts of data to fine-tune them to the specific problem at hand: 
 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
5     

 

 

RM(A) << 1 DM(A) >> 1 ηA << 1 

 

- One-Shot learning algorithms also require expert-domain features, but can use only 
a few examples to fine-tune the underlying model and have predictions: 
 

RM(A) << 1 DM(A) << 1 ηA ~ 1 

 

- Deep-Learning algorithms can automatically learn the most informative features, but 
require vast amounts of data to do so: 
 

RM(A) >> 1 DM(A) >> 1 ηA ~ 1 

 

- Endor’s algorithm uses Social Physics to automatically extract the relevant behavioral 
features from only a small sample of the data: 
 

RM(A) >> 1 DM(A) << 1 ηA >> 1 

 
2. Rate of Change of the Problem – another factor that influences the amount of data 

required to produce accurate prediction is the rate of change of the problem’s underlying 

model. Some problems are static, wherein their underlying parameters do not change, or 

change rarely. For example, faces in images do not change over months; faces are faces. 

On the other hand, underlying behavior patterns that lead to the churning out of a paid 

service may change over time, either gradually via social changes over months, or rapidly 

in a matter of days, as a response to a successful marketing campaign by a competitor. 

We quantify this dynamics as follows: 

 

𝝉 =  
𝝏𝑻

𝝏𝑴
 

 

τ is the problem’s persistence or tenacity, denoting the rate at which the underlying 

model changes, where ∂T represents the duration over which the model changes by ∂M 

bits.  

 

For example, τ  = 1 day / 10 bits means that the model changes drastically over a period 

of one day (i.e. fast rate of change) compared to τ = 1 month / 10 bits which refers to a 

much slower change of the model. Effectively, a dynamic model presents a different 

model every period, requiring re-training or re-learning the model. 

 

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
6     

 

 

 

Operational Implementation of Predictive Analytics: Chasing the Changing 

Model 

 

The main challenge in a feasible implementation of predictive analytics for a given problem is 

therefore obtaining enough information to cope with the behavioral change of the modeled 

objects. The amount of information bits that one can accumulate per one time unit is denoted 

as It.  

 

Operators of extremely-large social networks or search engines (e.g. Google or Facebook) can 

often accumulate vast amounts of information in relatively short periods of times. This is 

however impossible for the majority of the companies interested in predicting their 

customers’ behavior. 

 

In addition, even large players that acquire vast amounts of information every day would find 

it challenging to accurately models problems that are either (a) too complex, or (b) change 

too quickly, or of course (c) a combination of the former two. 

 

We can therefore point to a simple equation that determines the ability of a company to 

implement an operational predictive analytics solution. Companies that are able to satisfy this 

principle using the learning algorithms they employ and new data they continuously acquire, 

for the problems they are interested in predicting, will be able to successfully construct an 

operational process that achieves this goal, whereas companies who fail to do so, will not be 

equally successful. 

 

The Fundamental Operation Learning Principle: 

𝑰𝒕 × 𝝉 × 𝜼𝑨 > 𝟏 

 

The practical implications of this principle dictate that companies who fail in their 

attempts to construct an operationally functioning prediction systems should either: 

1. Improve their data collection bandwidth, acquiring larger amounts of relevant 

data per day; or 

2. Focus on more static problems; or 

3. Resort to more efficient learning algorithms. 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
7     

 

 

This simple relation means that one needs more information per day as the problem’s 

complexity increases and its persistence decreases as illustrated in the following chart: 

 

 

Here is how a few common predictive problems look when analyzed with this 

 

 

 Face recognition: This problem is characterized by a very low change rate, namely 

τ >>1, as generally speaking – faces do not change. Therefore, even for inefficient 

learning algorithm for which ηA is relatively small, It can still be very small, as one 

needs a lot of information to initially learn the problem, but not a lot of 

information to re-train it, since it does not change much. 

 

 Tracking a maneuvering mobile target: Tracking a moving target is characterized 

by τ << 1 since the fast movement of the target (or better yet – the fast changes in 

its trajectory or dynamic local changes that are introduced by its navigation 

algorithm to avoid detection) renders any non-immediate past data useless. 

However, such problems are also usually characterized by a large stream of 

incoming data, providing It ~ 1/τ  >> 1, which means that the dynamic nature of 

the target’s location is fully conveyed to the learning algorithm by the input data 

stream. This enables relatively simpler algorithms with ηA ~ 1 to solve the problem 

efficiently. 

 

 Human behavior: Human behavior is extremely dynamic, having τ << 1, meaning 

it contains elements that change relatively fast. Moreover, although companies 

who wish to predict human behavior can (and do) obtain additional information 

Easier for larger streamer of incoming data 

Ea
si

er
 f

o
r 

“s
ta

ti
c”

 p
ro

b
le

m
s 

τ [days/bit] 

I
t
 [bits/day] 

Easier for larger streamer of incoming data 

Ea
si

er
 f

o
r 

“s
ta

ti
c”

 p
ro

b
le

m
s 

τ [days/bit] 

I
t
 [bits/day] 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
8     

 

 

about their customers, it is always a partial derivation of the actual behavioral 

change. Metaphorically, this is similar to Plato’s Cave allegory, where the 

information perceived by our sensors passes through a very crude lens that 

captures only several aspects of it. In our formalism this translates to It  << 1/τ  , 

which subsequently means It ≥ 1. This means that in order to efficiently predict 

human behavior, one must employ extremely efficient algorithms of ηA >>1.  

 

Similarly, we can see how different solution techniques can be best used for each 

problem, modeled using this relation: 

 

 

The chart illustrates the previously mentioned Fundamental Operational Learning 

Principle  𝑰𝒕 × 𝝉 × 𝜼𝑨 > 𝟏.  The more static the problem is, and the more data about 

it we have – the closer we are to the upper-right corner of the chart (and the more 

accurate our predictions will likely be). And the closest we are to the upper-right 

corner – the less-efficient algorithms we need to employ to produce accurate 

predictions.  

Techniques that require vast amounts of information to train do not work well with 

dynamics problems, hence Deep Learning works best for problems whose underlying 

structure does not change fast, such as image processing and gesture recognition. In 

contrast, simple algorithms that can process information quickly, e.g. Kalman Filter, 

can deal with dynamics problems but require a high throughput of information to 

successfully predict. Traditional Machine Learning approaches such as Logistic 

Regression would be efficient in scenarios where we have relatively high volumes on 

incoming training data provided that the problems are fairly static as well.  

 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
9     

 

 

Social Physics as the Solution to the Never-ending Chase 

Endor tackles the need for constantly obtaining large quantities of data for changing 

models from two orthogonal points of view:  

 Transforming dynamic problems to a static model: The laws of Social Physics are 

immutable and agnostic to data-type and origin. Therefore, using them to project 

raw data into a Social Physics canonic representation space transforms the original 

problem into an instance of a new class of problem, whose underlying model is 

static (thanks to the inherent static nature of the Social Physics laws). This 

transforms the actual τ <<1 of the original problem to be extremely large τSP>>1.  
 

 Creating “Big Data” from Small Data: as mentioned above, when faced with the 

challenge of predicting their customers’ behavior, most companies face a major 

hurdle – the amount of relevant data they possess is often insufficient. This is 

specifically true for dynamic problems (frequent in marketing use-cases) or for 

problems that involve the introduction of a new element (such as predicting the 

response to a new product, or utilizing a new kind of input data). Endor transforms 

all the data streams it receives from its customers into a Social Physics canonical 

form, regardless of type, size and source. By using this canonical form, Endor is 

able to unify and consolidate all the data from all clients and all their queries into 

a single extremely large data base, growing at a consistently high pace (namely, 

with It >>1). This is then used to train a single, immutable deep-learning network 

that is trained on analyzing instances of Social Physics Canonical Forms (and not a 

specific query or customer). Hence, even if each customer provides very limited 

data, Endor is able to accumulate a vast amount of (canonically formed) data.  
 

Endor’s engine transforms the most difficult problems of human behavior 

predictions into slowly changing ones (via Social Physics), and to Big-Data (via the 

transformation to a canonical form), resulting in an “easy and efficient” problem, 

then solved using Deep Learning tools. This is illustrated in the following chart: 

 

 

Easier for larger streams of incoming data 

I
t
 [bits/day] Ea

si
er

 f
o

r 
“s

ta
ti

c”
 p

ro
b

le
m

s 

τ [days/bit] 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
10     

 

 

2.2. SOCIAL PHYSICS: OVERVIEW 

 

In the previous section we have briefly described how Social Physics can be used to overcome 

the inherent challenge of predicting human behavior, using semantics agnostic static 

mathematical invariances. In order to better understand how this is feasible, consider physical 

laws – for example Newton’s second Law or the Law of Momentum Conservation. Any object 

maintains its initial path unless an external force acts upon it. Note that in order to deduce the 

existence of such hidden force there is no need to “learn the data”, understand its statistical 

attributes, or test many systems that behave in a similar way. Since the physical law is a given, 

any violation of it is abnormal and can immediately be detected and interpreted as the 

outcome of some “invisible hand.” If one detects objects that suddenly change their direction, 

it is possible to immediately deduce that a force has acted upon them. If their change is similar, 

then it is most probable that the same force was exerted on all of them. This simple realization 

is only possible due to the understanding of the physical law.  

 

While Social Physics is far less absolute and rigorous than physical laws, the concept is similar. 

If something violates Social Physics laws, it can immediately be qualified as “interesting,” 

being the data manifestation of some valuable property or attribution in the real world. This 

does not require learning, benchmarking, baselines or any other data science or machine 

learning tools. A violation of the social physics laws can be detected extremely fast and in a 

very robust way, regardless of the data type that generated it. 

 

2.3. ENDOR: POWERED BY SOCIAL PHYSICS 
 

2.3.1. Data Transformation into a Canonical Representation  
 

By extracting the previously discussed “behavioral clusters” (namely, the detected violations 

of Social Physics invariances) and aggregating them into a “Knowledge Sphere,” the raw input 

data (of whatever shape or form, as long as it originates from human actors) is being 

transformed into a canonical form. This form represents clustering of people who violated a 

Social Physics Law “together”, in other words – people that display a “too high synchronous 

change” in their behavior, vis-à-vis a certain invariance. This is akin to physical objects that 

would change direction at a specific time in a similar manner – while the force that caused 

these changes is not visible, we can deduce with high likelihood that the objects were all 

affected by a single source. Similarly, Endor’s canonical representation of data in the form of 

behavioral clusters contains groups of people who most probably were influenced by the 

same hidden “social forces” and thus share common “real world traits”. Whenever new raw 

data is available, it is sent to Endor by the customer (usually on a daily or weekly basis), 

allowing for additional behavioral clusters to be automatically extracted. 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
11     

 

 

The benefit of this representation is threefold: 

- Automatic: using the mathematical invariances of Social Physics, as any data that 

originates from human behavior can automatically be transformed into a collection 

of behavioral clusters, regardless of the type of input data, that does not need to be 

declared or analyzed (e.g. phone call records, credit card purchases, taxi rides, or any 

other type of proprietary data the customer may possess). This, combined with an 

extremely high resilience to noises and gaps in the input data, means that the process 

of transforming dirty raw data of an unknown type  into uniform behavioral clusters 

becomes, for the first time, fully automatic (see “Robustness to Noise” section for 

additional information). 

 

- Uniform: by stripping the data of any domain, demographics or semantics aspects, 

the remaining information containing the behavioral clusters is ideally shaped for the 

following “querying” phase. In fact, using this uniform representation, Endor can 

“create big data” when there is none, allowing the querying phase of the process to 

utilize deep-learning techniques that were impossible for the original owner of the 

data! This is enabled as the Endor deep-learning engine has access to behavioral 

clusters originating from many types of data, and from many customers, all 

transformed into a single-form. 

 

- Emerging trends: simply put, the Social Physics invariances describe the way certain 

statistical properties of human crowds evolve over time. This time-oriented aspect 

enables Endor to easily detect emerging behavioral changes – dynamics that only 

recently occur, and that in most cases did not have enough time to generate enough 

observable data that would enable them to be detected with a high enough statistical 

significance using traditional methods. In addition to the fact that through the use of 

Social Physics these additional signals can be detected, these are usually the very 

signals that are of high importance for a variety of business questions – as they 

contain information about recent trends. 

 

“Old-school” Machine Learning worked with pre-defined features and could extract relevant 

information from relatively small amounts of generic data. However, much of the results 

depended on the selected features. Deep Learning identifies the most relevant features by 

itself, but requires huge amounts of data. Each data type and question asked require finding 

the relevant features again, thus requiring more data. 

Social Physics transforms any type of human behavior data to a canonical form of human 

clusters based on their behavior. This works with both small and large amounts of data. In 

addition, thanks to the Social Physics’ canonical form, Endor can ingest all data types and all 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
12     

 

 

questions, regardless of data size, and generate one huge human behavior data set that uses 

the power of Deep Learning to answer any question. 

 

 

 

Current predictive analytics paradigm (top charts) vs. new paradigm enabled 

 through Social Physics (bottom charts). 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
13     

 

 

2.3.2. Querying the Canonical Representation (“Knowledge Sphere”) 

As the Knowledge Sphere contains the overall information detected about all users, it can 

provide predictions for any question with the ease and speed of a simple data search: The 

initial creation of the Knowledge Sphere usually takes 1-4 hours for a typical data consisting 

of a billion records. After this process is complete, the same Knowledge Sphere can be used 

to answer dozens of questions, in minutes. There is no need for prior domain knowledge or 

extraction of relevant features.  

 

A query is submitted by providing an “example” (positively labeled IDs), of any size. Endor’s 

engine then uses the Knowledge Sphere to generate an answer: a ranked list of users, most 

probable to be “behaviorally similar” to the relevant query. There is no “training” / “learning” 

phase; there is no need to “interpret the result.” The equivalent to this automatic process for 

each specific question would have required anything from weeks to months of effort for data 

scientists using conventional methods. 

 

For example, a query regarding users who are mostly likely to churn in the near future is 

described by a list that contains the identity of previous churners. Alternatively, a query that 

tried to identify which new customers are likely convert to a premium account would be 

described by a list containing customers who recently converted to this premium service. 

Both queries, however, would use the same Knowledge Sphere, requiring no re-training of 

any sort.  

 

Notice that whereas most of such behavioral predictive questions would be extremely hard 

to resolve using Deep Learning (due to lack of sufficient data, and the frequent change in the 

underlying model), Endor circumvents this issue by using the collection of all Knowledge 

Spheres and queries. Thus, Endor “creates” the big data that is required for its internal Deep 

Learning component, which is in charge of providing the actual predictions. These predictions 

are based on the particular Knowledge Sphere, even if it is based on an extremely small 

amount of data. 

 

The following chart illustrates the flow of traditional Deep Learning (upper flow) in 

comparison to Endor’s one (lower flow). For non-human behavioral data (e.g. millions of 

images) Deep Learning can likely produce high quality predictions, given a proper training 

done by a data expert – as current Deep Learning tools are designed to be used by Engineers, 

who are in addition, experienced with the use of such tools. Human data on the other (e.g. 

taxi data) has an underlying model that changes so frequently that it cannot be easily resolved 

by Deep Learning modeling (see Section 2.1 for an in-depth discussion on this issue).  

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
14     

 

 

However, when transformed into the form of a set of behavioral clusters, taxi data is stripped 

of its semantics and becomes virtually identical in representation to behavioral clusters 

obtained from phone call records, credit card purchases, or any other type of human data. In 

addition, as there are many different customers who contribute data of each of those types 

(namely, many e-commerce platforms, each uploading their own purchase and web-activity 

data), training a Deep Learning model now becomes a possibility. This is enabled as the model 

would not be trained on the raw data or problems, but on the multitude of behavioral clusters 

– a large collection of data in a uniform representation, that is also characterized by a static 

underlying model (i.e. the Social Physics Laws, that are mathematical invariants and are 

therefore static, compared to the behavioral dynamics of the raw data, that is highly 

dynamic). 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
15     

 

 

 

2.4. DATA SECURITY AND ANONYMITY 

 

2.4.1. Data Stored by Endor 

In addition to its ability to automatically resolve an endless variety of behavioral prediction 

questions, Endor’s solution provides a high level of data security and the ability to anonymize 

any required data field. As detailed in the previous sections, the prediction is done using the 

Knowledge Sphere – a collection of semantics-free behavioral clusters, containing simply a 

large number of groups-of-users (each guaranteed to have a common social or behavioral 

trait). It is easy to see that any sensitive or personal information, if such information existed in 

the original raw data, is no longer part of any data used by the system at this stage. The only 

information that is available to the system is IDs of users, as shown in the following example: 

 

Behavioral_cluster1 =  (ID1, ID17, ID23, …) 

Behavioral_cluster2 =  (ID142, ID4287, ID9711, …) 

… 

Behavioral_cluster748,329 =  (ID5, ID37, ID218, …) 

 

It should be noted that even this information can easily be hidden by hashing the IDs at the 

raw-data level by the customer, upon data onboarding (see below). 

 

2.4.2. Data Onboarding to Endor 

As presented in previous sections the methods used for extracting the behavioral clusters 

from raw input data rely on the detection of groups of users who display data-patterns that 

violate a Social Physics invariance. This is done by tracking the dynamics of certain statistical 

properties that portrays the synchronous nature of the activity of the users. For tis 

implementation, the aforementioned does not require the actual values contained in the 

data, but alternatively can be done by a fully hashed replacement. This enables the customer 

to provide Endor with a fully hashed dataset, while still benefitting from its superior predictive 

capability. In addition, as Endor is 100% semantics agnostic, the names of the data-fields can 

be hashed as well.  

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
16     

 

 

An example of such hashing for financial records appears below: 

 

 
Header before hashing: 
 

 
ACCOUNT 
NUMBER 
 

 
BRANCH 
 

 
GENDER 
 

 
TYPE OF 
TRANSACTION 

 
DESTINTATION ACCOUNT 

 
Data records before hashing: 
 

 
183972 

 
291/30 

 
Male 

 
Transfer 

 
382732 
 

 
183972 

 
291/30 

 
Male 

 
Balance Inquiry 

 
N/A 
 

 
382732 

 
291/30  

 
Female 

 
Transfer 

 
439001 
 

 
… 
 

 
Header after hashing: 
 

 
Field1 
 

 
Field2 
 

 
Field3 
 

 
Field4 
 

 
Field5 
 

 
Data records after hashing: 
 

 
AjF32sdx 

 
Q2KPbv3A 

 
Wsqp289X 

 
q8Vb3MAs 

 
Je2qx92n 

 

 
AjF32sdx 

 
Q2KPbv3A 

 
Wsqp289X 

 
q8Vb3MAs 

 
x3PNm78A  
 

 
Je2qx92n 

 
Q2KPbv3A 

 
m28SbA12 

 
q8Vb3MAs 

 
yL19B4GQ  
 

 
… 
 

 

 

 

 

 

 

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
17     

 

 

2.5. SOCIAL PHYSICS: MATHEMATICAL EXPLANANATION  
 

2.5.1. Framework 

We first introduce the basic principles of Endor’s engine with their generic mathematical 

formalism. This is followed by two examples of possible implementations: (1) computer-

vision-oriented and (2) social-graph-based. The first example demonstrates Endor’s drawback 

when used with sensor-based data (that is human-unrelated), whereas the latter illustrates 

the concepts of Social Physics, and its benefits in predicting human behavior.  

 

Note: throughout this discussion we provide numerous mathematical illustrations for the principles of 

Social Physics and the way it is used by the Endor engine. However, certain mathematical details 

regarding the Social Physics Laws were omitted from this discussion due to IP considerations. 
 

Let 𝑑(𝑥, 𝑡) be a temporal data stream, where x represents a single data point.  

Let 𝐿(∙) be a Law Operator which transforms the raw data 𝑑(𝑥, 𝑡) into a Law Representation: 
 

    ,
1

,
| || |

X T
X T

L d L d x t dxdt
X T

  
 

 

The Law itself is formulated as an equation that equates the Law Operator to an a-priori 

constant C (which can be a number, a distribution class, such as a Power Law, etc.). This C 

represents the invariant represented by the Law.  
 

For the purpose of illustration, we can imagine a hypothetical Law that represents the 

understanding that the change of the output of a white noise signal over time is limited to a 

very small threshold. A violation of this Law would take the form, for example, of a signal that 

displays a sudden strong output-spike. A mechanism that filters signals and can detect such 

anomalous outputs can then be built, in order to find violations of this Law. Naturally – using 

such a mechanism would only make sense in the case of signals known to be governed by the 

Law in question (i.e. origins of white noise), as for other signals such spikes cannot be classified 

as “violations”. 

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
18     

 

 

Another example can be an X-ray machine, built in order to detect anomalous “chunks” of 

radiation-absorbing materials. In this example the X-ray device (and the technician operating 

it and deciphering the resulting images) serve as a “violation detector” for analyzing 2-

dimensional information streams and locating violations of an invariance that asserts that a 

coherent X-ray beam that hit a film should create an image of equal absorption (more or less, 

depending on the quality of the film, and the coherence of the beam).  

This invariance, or Law, is a known physical fact, and it is used for medical applications by 

artificially inducing coherent X-ray beams onto high-quality films, while passing through a 

third substance – with the aspiration of finding “violations” that would indicate the existence 

of an X-ray absorbing element in this substance. Bones for example are such a material, 

representing in this case a “real world phenomenon” that is detected in the “data” (that is the 

exposed film), manifested by its unequal absorption of radiation, that in itself is a violation of 

the aforementioned Law. 

 
 

Concrete formal mathematical examples for the implementation of the above principles 

appear in Sections 2.5.2 and 2.5.3. 

In the case of Social Physics such violations would comprise a group of people, having 

generated a certain dynamic in the input data, analytically known to be statistically highly 

improbable, under the assumption that the input data was originated from human activities, 

and therefore adheres to the Social Physics Laws. 

 

2.5.2. Validating that a Data-Subset is a Violation of Laws 

Endor’s engine implements Law Operators and Constants of several Social Physics Laws (the 

specific details of which are not described due to IP considerations). In this section we describe 

the validation of a Law Violation. It is important to note that as in many other computation 

problems, the validation that a specific signal consists of a Law Violation is fundamentally 

different to the detection of such interferences. In this section only the former is discussed, 

namely – given the validation whether a “potential interference” is indeed a Law Violation or 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
19     

 

 

not. The details of Endor’s search algorithm that can efficiently scan a large scale high-

dimensional data source and perform such on-the-fly validation will not be discussed here. 
 

The mathematical formulation of the Laws we validate is always given in the following form: 

 ,X TL d C  
 

Given the explicit formulation of a Law, local deviations from it can be validated by measuring 

their deviation from it, denoted by ξ, as follows: 

   ,, x tx t L d C       
 

Here ∆x represents a subspace of X, whereas ∆t represents a temporal window. This deviation 

can thus be calculated for every subspace of X and any period of time, and generates a 

measure of how much that subspace violates the relevant Law, during the given time period.  

 

By comparing this measure to a pre-defined threshold ξthreshold the subspaces that violate the 

Law can be detected: 

    : , thresholdr t x x t        
 

The violation threshold ξthreshold is selected such that the spontaneous emergence of a signal 

that would defer from the Law further than the threshold is highly improbable. This enables 

automatic verification that a certain data subset is a violation of a Law, with a high-enough 

statistical significance, without any prior knowledge of the semantics of the data itself. 

 

Notice that as the signal changes both in time and space, different temporal windows can 

create different subspaces that are detected as Law Violations. Endor uses a pre-defined fixed 

set of temporal windows that (derived from the Laws and not from the data) : ∆t  = 1-day, 7-

days, 30-days, 90-days.  

 

If the data is highly dynamic, the longer temporal windows are unlikely to generate any 

deviation groups; if the data is static, the shorter temporal windows are unlikely to generate 

any deviation groups. Regardless, none of the windows generate “junk-groups”, because by 

definition – noise cannot generate a consistent Law Violation (or in more formal terms, the 

probability that noise will generate a large enough violation of the Law, is close to zero, due 

to the fact that this is the way the threshold ξthreshold is selected). 

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
20     

 

 

The Knowledge Sphere is an aggregation of all group deviations from all Laws, for all relevant 

temporal windows: 

  : ,sphereK r t t L    
 

This Knowledge Sphere is calculated once per data-set, as this process is unaffected by the 

queries being asked, but rather the internal behavioral structure originating from the raw 

data. From an abstract point of view, Endor’s Social Physics engine “compresses” the 

anonymized raw data into behaviorally relevant canonical representation. 

 

 

2.5.3. Examples 

Following are two examples that demonstrate the use of Laws for detection violating-patterns 

in data. By way of illustration we use known mathematical phenomena for demonstrating 

this mechanism. 

 

Example 1: Vision  

Let d(x,t) denote the color of a specific pixel x at a specific frame t. We may now define the 

Same Color Law, dictating that every color of any sub-region must be some pre-defined color 

C: 

    ,
1

, ,
| || |

X T
X T

L d x t d x t dxdt
X T

  
 

 

This Operator takes region x and time window t and calculates the average color for this input 

data. 

 

This example illustrates the concept of Laws by negation: there is no inherent a-priori known 

law about pixels in a video (certainly not one that assumes every sub-region is of the same 

average color…). Considering this Low Operator and a constant C = Light Blue, we can now 

define ∆x as a square window of size N×N pixels, and use a single frame ∆t = 1.  

This yields the following as the local deviation of N×N windows from Light Blue: 

   ,, x tx t L d C       
 

In 2-dimenional images this can easily be applied to measure the deviation of patches from 

Light Blue, comparing this deviation to a pre-defined threshold. Patches whose deviation 

surpass this threshold will be classified as “clusters of pixels with a similar property”, 

representing all the pixels in the square that violated the “Light Blue law”. 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
21     

 

 

We can illustrate this using a slightly more generic hypothetical Law: let’s assume that images 

are always “smooth” (namely, they are monotonous in smaller scales, lacking “peaks”, or local 

maxima\minima). Under this assumption, we can observe the following “data” – an image 

that contains a “lawful” background of smooth color, with two patches (one Blue, and the 

other Red). A sampling of a large number of small square-shaped random regions would easily 

locate these two “violations”:  

 

 

Then, a “query” can be asked, in the form of Red pixel. Such a pixel would be identified as 

being contained in the Red violation, returning the Red patch as the result. 

 

Note again that the intention of these examples is to illustrate the mathematics and 

mechanics of Social Physics rather to suggest that it is advantageous for a simple vision-based 

application, as such applications can be well addressed with traditional computational vision 

or deep learning methods. 

 

 

Example 2: Scale-Free Networks 

In this example 𝒅(𝒙, 𝒕) abstractly represents a graph with x being the graph’s nodes. The Law 

Operator is the degree-distribution operator, formulated as: 

   
1  has degree 

0 otherwise
n

x n
L x l x


  

  

 

This vector operator generates 1 for the degree of each node. The summation of the result of 

this operator over all the graph’s nodes yields a cardinality vector for the graph’s degrees 

(equivalent to the degrees distribution, when dividing by the number of nodes). 

 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
22     

 

 

In this example we shall assume that the graph is a Scale-Free network. Therefore a Law 

Constant that assumes the power-law degree distribution can be applied (for some 

normalization constant 𝛼): 

𝐶̅ = 𝑐𝑛 = 𝛼 ∙ 𝑛
−𝛾 

 

This Law Constant can then be formulated as: 

    , ;
1

,
| || |

X T n n n
X T

L d L d x t dxdt C
X T

  
 

 

This Law implies that the overall graph should obey a power law distribution of the degrees 

of all its node. However, in many large real-world scale-free graphs there could be significant 

local deviations from such distribution. This may occur for example around cliques (i.e. fully-

connected sub-graphs) or chains (i.e. sub-sets of the nodes that form a connected tree with 

no node having more than 2 neighbors). Such deviation, or violations, of the Law are 

illustrated in the following chart, containing their manifestation in both a structural 

representation (left) and as an adjacency matrix (right): 

 

 

Note that given the Law, such violations can easily be validated by a variety of measures, such 

as: 

   
2

, ;, x t n n
n

x t L d C     
 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
23     

 

 

This deviation measures the cumulative square of the differences (whereas another example 

for such measure can be the KL-divergence of both probability distributions). Here, Δ𝑥 

represents all possible subgraphs of the graph. Obviously, scanning all possible subgraphs in 

an input graph is not feasible, as it is a member of a class of “difficult problems” known as 

“Non-Polynomial Hard problems”. In this sense, it is important to distinguish between 

validating a Law Violation (that requires knowing the details of the Law) and detecting a Law 

Violation (that requires a set of proprietary techniques that are specifically developed for each 

Law). Such detection techniques are part of Endor’s proprietary technology, tailored-made 

for the Social Physics mathematical laws. 

 

Returning to our scale-free example, assuming we possessed an efficient technique for finding 

such local interferences in graphs, they would have resulted in a collection of sub-graphs that 

can be formulated as follows: 

    : , thresholdr t x x t        
] 

And the Knowledge Sphere implied by this Law is defined as: 

  : ,sphereK r t t L    
 

Once a Knowledge Sphere containing the collection of violations of Social Physics Laws is 

available, it can be used to detect “lookalikes” for any given labeled exemplar, defined as a list 

of objects from the same domain. In our example, given a list of graph nodes all of the other 

graph nodes can be scored according to how many clusters they share with the labeled 

exemplar. Alternatively, different scoring metrics can be used, as long as they solely rely on 

the detected clusters and the labeled list as input, in order to produce an output in the form 

of a population scoring. We refer to such metrics, or scoring mechanisms as “Scorers”, and 

will elaborate on these in the next section. 

 

Note that different temporal windows can generate different Knowledge Spheres, 

representing very different associations among the graph nodes. In addition, these clusters 

represent behavioral connections that are not generated from external data sources such as 

social media or social networks, but rather from the customer’s own internal transactional 

data source(s). This enables Endor to detect implicit behavioral clusters that are not explicitly 

manifested in any available data. 

 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
24     

 

 

2.5.4. Answering Questions: Simple “Scorer” 

Once the Knowledge Sphere is available the user can start asking business-relevant questions 

by providing Queries – lists (possibly very small) of labeled data with semantic meaning: 

𝑦 ∈  𝑋. 

 

In this section we give two examples of “Scorers” – functions that use the behavioral clusters 

+ query to generate a ranked population list as output.  

 

The first scorer we discuss is a simple co-clusters aggregation scorer that for each candidate 

�̃�  ∈  𝑋 calculates the following score:  

1 ,

0 otherwise
sphere

y

x K

y x y x
score

 

 
 




 
 

Each possible “candidate” is scored according to the number of clusters in the Knowledge 

Sphere that they co-inhabit with members of the labeled data. This simple scorer aims for 

counting the number of behavioral similarities of the members of X with the input labeled list. 

The list of objects that share clusters with objects in the labeled input list is represented as: 

 : , spherey x x y x x K      
 

For example, given a colored point and the “Same Color Law” from Example 1, the colored 

area around a point that is given as input. In the “Scale Free Graphs Law” given a node the 

output would be the nodes in all subgraphs containing that node that locally violate the 

degree distribution. 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
25     

 

 

2.5.5. Answering Questions: Deep Learning Scorer 

The scorer discussed in the previous section is an intuitive example of the way the clusters 

comprising the Social Physics Knowledge Sphere can be used in order to produce high quality 

“lookalikes prediction” for any “examples input” (i.e, a Query), without knowing in advance 

the nature of this query. This simple example illustrates the advantages of the social physics 

canonical data representation. However, in order to produce an accurate prediction Endor 

developed a more robust Scorer based on state-of-the-art deep learning algorithms.  

 

As noted above, where human behavior is concerned, an efficient use of deep learning 

requires vast amounts of data for its training phase. This is hard to achieve using the raw data, 

as each labeled query usually contains a small number of labels, and there are typically only a 

handful of instances per query, and also a relatively small number of different queries per 

client. This type of complex, dynamic and small-sized label data is not well suited to deep 

learning. Endor’s Social Physics engine overcomes this limitation by transforming all datasets 

from all clients, and all instances of all the queries into a large collection of a single canonical 

form: Knowledge Sphere that is composed of clusters of people, and examples that refer to 

these clusters. Combining all data from all these sources enabled Endor to create a large 

enough labeled training-set to train a deep-learning network that scores each person, given 

the labels and the Knowledge Sphere. This process is done once, resulting in a high-quality 

trained deep-learning model, that can then be used to efficiently process any new clusters set 

that is produced from new data sources, and new queries. 

 

Note that this trained model does not need to be periodically retrained – not for new queries, 

nor for new types of datasets, as it was trained on (many) instances of data represented as a 

collection of clusters (i.e. the Social Physics canonical representation form). Using this method, 

Endor combines the advantages of both Social Physics and Deep Learning: Social Physics 

transforms anonymized raw data into canonical form based on violations of Social Physics 

Laws, whereas Deep Learning algorithms then score candidates based on the created 

Knowledge Sphere and the (possibly few) labeled data. 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
26     

 

 

This flow is illustrated in the following charts: 

 

 

 

 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
27     

 

 

2.6. ROBUSTNESS TO NOISE 

 

One of the main strengths of the Endor predictive platform is its high resilience to data-gaps 

and noisy data. Traditionally, every data-analytics project begins with a “data sanitation” 

phase, consisting of the detection and desired mitigation of undesired data segments such as: 

- Data gaps (i.e. periods of missing data, either full or partial) 

- Gibberish insertions in the raw data 

- Semantic ambiguities, such as a category name that may appear in the data in more 

than one form 

- Normalization issues of numeric values 

- Required binning of numeric values 

- … 
 

Machine Learning algorithms are usually highly susceptible to noise and given that data is 

normally noisy, the phase of mitigating those data problems is typically costly and 

protracted.  The reason for this is that whereas in the real world data is generated in its raw 

form (financial transactions, phone calls, etc.) in order for it to be analyze-able by traditional 

Machine Learning techniques it must be converted to an aggregative form, referred to as 

“features”, or “properties”. This aggregation can in turn be significantly affected by a small 

number of wrong data values, or different quantities of values for different users. 

Endor overcomes this requirement by analyzing the raw data itself (as described in detail in 

the previous sections). In addition, Endor’s engine does not perform statistical analysis of the 

data in the search of patterns that can be used for prediction, but rather – uses the Laws of 

Social Physics – mathematical invariances, external to the data, and unaffected by it. This 

approach has several significant advantages, stemming from the following basic notion: 

 

Noise cannot create a data-pattern that “cannot arbitrarily emerge” 

(where the latter is defined as data-patterns that can analytically be shown to exist in negligible probability) 

 

In order to understand how this observation allows for the automatic extraction of data 

insights from any human-data without any prior cleaning, let us recall that the extraction 

process searches for groups of objects in the data that violates one of the Social Physics Laws. 

Namely, groups that display data patterns that we can prove cannot naturally emerge in the 

data (using the mathematical analysis enabled by Social Physics). 
 

This means that whereas noise can indeed hide insights from our engine, it can, by definition, 

(almost) never create a data pattern that would be detected as a Law violation. Noisy data 

cannot violate a Social Physics Law, only human-driven signal data can. 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
28     

 

 

3. RESULTS 

 

This section presents a variety of use-cases that illustrate how the Endor prediction system 

can be utilized. A detailed description of the overall prediction process, using data which 

comprises 7 days’ worth of activity of a large financial investment platform to accurately 

answer 4 different predictive questions (including a comparative analysis with Google’s 

Tensor Flow deep learning platform); 

1. An example of using Endor in a fully automatic way in order to crack a Kaggle 

Challenge. 

2. A Coca-Cola case-study in which Endor provides accurate predictions for a multitude 

of business questions using point-of-sale data, in less than 24 hours. 

 

 

3.1. USING FINANCIAL ACTIVITY OVER 7 DAYS TO ACCURATELY AND 

AUTOMATICALLY ANSWER FOUR PREDICTIVE QUESTIONS 
 

In this section we demonstrate the overall prediction process using the Endor system: 

- Description of the raw data used 

- The transformation of the data to the Social Physics representation (namely, the 

knowledge sphere that comprised a set of behavioral clusters) 

- The definition of four  predictive questions 

- The manifestation of the queries in the Social Physics form 

- The overall predictive accuracy 

- A comparison to Google’s Tensor-Flow deep learning platform. 

 

 

3.1.1. Data: 

The data that was used in this example originated from a retail financial investment platform 

and contained the entire investment transactions of members of an investment community. 

The data was anonymized and made public for research purposes at MIT (the data can be 

shared upon request). 

Following is an overall summary of the dataset: 

- 7 days of data 

- 3,719,023 rows 

- 178,266 unique users  

- No contextual or semantic interpretation of the data was given 

- 12 data fields: 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
29     

 

 

 

 
FIELD NAME 
 

 
TYPE 
 

 
# UNIQUE VALUES 

Time1 Time 501573 unique 

Time2 Time 4 unique 

User ID INT32 Categorical 178266 unique 

Record ID INT32 Categorical 1053574 unique 

Property1 INT32 Categorical 24 unique 

Property2 INT32 Categorical 7 unique 

Property3 INT32 Categorical 134 unique 

Property4 INT32 Categorical 77527 unique 

Property5 INT32 Categorical 10 unique 

Property6 INT32 Categorical 27 unique 

Property7 INT16 Categorical 9 unique 

Property8 Double     Numeric 3772 unique 

 

 

The following important aspects of the dataset should be noted 

 The data was given in its raw non-aggregated form, and contained events on a user-level. 

 No contextual or semantic interpretation of the data was provided. 

 No data-sanitation was done. The data contained noises, gaps, duplicate records, and so on. 

 The data contained an extremely uneven distribution of records-per-user (78% of the users has 10 

or less transactions, but there are 4,000 with >200 records. Median is 4 records per user): 

 

 

 

  

10
0

10
1

10
2

10
3

10
4

10
5

10
6

10
0

10
1

10
2

10
3

10
4

10
5 Log-log representation of the number of records per user

Users

R
e

c
o

rd
s
 p

e
r 

u
s
e

r



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
30     

 

 

3.1.2. Automatic Clusters Extraction 

Upon first analysis of the data the Endor system detects and extracts “behavioral clusters” – groups of 

users whose data dynamics violates the mathematical invariances of the Social Physics. These clusters 

are based on all the columns of the data, but is limited only to the last 7 days – as this is the data that 

was provided to the system as input.  

Following is a summary of the behavioral clusters from the dataset that were detected by the system: 

 

Number of 

clusters: 

 
268,218 

 

 
Clusters sizes: 
 

 
62 (Mean), 15 (Median),  
52508 (Max), 5 (Min) 

 
Clusters per 
user: 
 

 
164 (Mean), 118 (Median),  
703 (Max), 2 (Min) 

 
Users in 
clusters: 
 

 
102,770 out of the 178,266 users 

 

Records per 
user: 
 

 
6 (Median), 33 (Mean): applies only 
to users in clusters 
(Compared to 4 and 21 in general 
population) 
 

 

 

3.1.3. Prediction Queries 
 

The following prediction queries were defined: 
 

 New users to become “whales”: users who joined in the last 2 weeks that will generate at least 

$500 in commission in the next 90 days 

 Reducing activity : users who were active in the last week that will reduce activity by 50% in the 

next 30 days (but will not churn, and will still continue trading) 

 Churn in “whales”: currently active “whales” (as defined by their activity during the last 90 days), 

who were active in the past week, to become inactive for the next 30 days 

 Will trade in Apple share for the first time: users who had never invested in Apple share, and 

would  buy it for the first time in the coming 30 days 

 

As can be seen, all of the above questions refer to data extending beyond the 7 days dataset – both 

past data (used to generate the “search population”, or “examples”), and future data (used for 

validation of the predictions). In order to avoid providing the system with any information external to 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
31     

 

 

the 7-day period, the queries were formulated in the form of lists containing users_IDs values. For 

example: 

- Query name: “New users to become “whales” 

- Search population: a list of user_IDs containing users who joined in the past 2 weeks (prior to 

End_of_data). 

- “Examples”: a list of user_IDs containing users known to be “whales” (i.e. users who 

generated more than $500 in commission in the 90 days prior to End_of_data). 

- List of targets that need to be found: a list of user_IDs, that is a subset of “Search population”, 

containing users who generated more than $500 in commission in the 90 days after 

End_of_data. This list was only used for validation purposes, and was not provided to the 

system. 

 

 

3.1.4. Knowledge Sphere Manifestation of Queries 

It is again important to note that the definition of the search queries is completely orthogonal to the 

extraction of behavioral clusters and the generation of the Knowledge Sphere, which was done 

independently of the queries definition. Therefore, it is interesting to analyze the manifestation of the 

queries in the clusters detected by the system: do the clusters contain information that is relevant to 

the definition of the queries, despite the fact that: 

 The clusters were extracted in a fully automatic way, using no semantic information about the 

data, and 

 The queries were defined after the clusters were extracted, and did not affect this process. 

 

This analysis is done by measuring the number of clusters that contain a very high concentration of 

“samples”; In other words, by looking for clusters that contain “many more examples than statistically 

expected”. A high number of such clusters (provided that it is significantly higher than the amount 

received when randomly sampling the same population) proves the ability of this process to extract 

valuable relevant semantic insights in a fully automatic way.  

 

The following table illustrates this observation, comparing the number of behavioral clusters that 

contain a certain amount of “Samples”, compared to the number of “Random Clusters” that contain 

the same amount of samples. Random clusters refers to a set of N groups of users randomly sampled 

from the customers population, such that N equals the number of behavioral clusters detected by the 

system, and that the sizes of these randomly sampled groups equals the sizes of the clusters detected 

by the system. This is used to demonstrate the information encapsulated by the behavioral clusters, as 

they contain significantly more clusters of high consistency of “target users” compared to the random 

samples (and recalling they were detected by the system prior to the definitions of the “questions”). 

 

The number of “samples” requested is given in units of “Baseline”. Namely – “X5 of baseline” for the 

“Reducing Activity” query (for which the baseline is approximately 11%) means clusters that have 55% 

overlap with the “samples” (namely, that 55% of their members also appear in the list of “Previous 

users who reduced activity”). 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
32     

 

 

 
Clusters containing many target customers 
 

 
#Random Clusters 
 

 
#Behavioral Clusters 
 

 
Reducing activity 

 
X2 from baseline 

 
0 

 
98 

X5 from baseline 0 11 

Churn in "Whales" 
 

X2 from baseline 212 1678 

X5 from baseline 67 525 

X10 from baseline 21 114 

Never Bought 
 

X2 from baseline 3962 60044 

X10 from baseline 1332 25542 

X20 from baseline 415 9090 

New Whales X2 from baseline 38 1898 

X5 from baseline 0 65 

 

 

3.1.5. Prediction Results 
 

The following table illustrates the accuracy of the predictions for the four queries 

 Baseline: the average portion of requested target customers in a random sample of the 

population, representing the accuracy of a random guess.  

 Candidates: the size of the search population. For example, in the “New users to become whales”, 

the number of candidates refers to the number of new users. 

 Top-100: the portion of requested targets customers in the top-100 members of the prediction 

report (similarly, for Top-250 and Top-500). 
 

  
Baseline 
 

 
Top 100 

 
Top 250  

 
Top 500 

 

New users to become "Whales" 
Users who joined in the last 2 weeks that will generate 
at least $500 in commission in the next 90 days 
 

 
7.5% 
(170 out of 
 2270 candidates) 

 
37% 
 

 
28.8% 
 

 
21% 
 

Reducing activity 
Who among the current active users will reduce 
activity by 50% in the next 30 days (but will not churn) 
 

11.4% 
(255 out of 
 2233 candidates) 

21% 
 

23% 
 

20.2% 
 

Churn in "Whales" 
Currently active "whales", who were active in the past 
week, to become inactive for the next 30 days? 
 

1.66% 
(69 out of 
 4141 candidates) 

10% 
 

10.8% 
 

9.2% 
 

Will trade in Apple for the first time 
Users who had never bought Apple share, and will buy 
it for the first time in the coming 30 days. 
 

0.5% 
(839 out of 
 161382 candidates) 

14% 
 

12% 
 

10% 
 

As can be seen, and as expected, accuracy decreases as we reach deeper to the predictions list. 



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
33     

 

 

3.1.6. Comparison to Tensor-Flow 

 

In this section a comparison between prediction results obtained by the Endor system and Google’s 

Tensor Flow is presented. It is important to note that Tensor Flow, like any other Deep Learning library, 

faces some difficulties when dealing with data similar to the one under discussion: 

 An extremely uneven distribution of the number of records per user requires some canonization 

of the data, which in turn requires: 

o Some manual work, done by an individual who has at least some understanding of data 

science. 

o Some understanding of the semantics of the data, that requires an investment of time, as 

well as access to the owner or provider of the data 

 A single-class classification, using an extremely uneven distribution of positive vs. negative 

samples, tends to lead to the overfitting of the results and require some non-trivial maneuvering. 

This again necessitates the involvement of an expert in Deep Learning (unlike the Endor system 

which can be used by Business, Product or Marketing experts, with no perquisites in Machine 

Learning or Data Science). 

 

We have asked an expert in Deep Learning to spend 2 weeks crafting a solution that would be based 

on Tensor Flow and has sufficient expertise to be able to handle the data. The solution that was created 

uses the following auxiliary techniques: 

- Trimming the data sequence to 200 records per customer, and padding the streams for users 

who have less than 200 records with neutral records. 

- Creating 200 training sets, each having 1,000 customers (50% known positive labels, 50% 

unknown) and then using these training sets to train the model. 

- Using sequence classification (RNN with 128 LSTMs) with 2 output neurons (positive, 

negative), with the overall result being the difference between the scores of the two. 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
34     

 

 

The table below compares the results obtained using these techniques (red) to Endor’s predictions 

(blue): 

 

  
Baseline 
 

 
Top 100 

 
Top 250  

 
Top 500 

 
New users to become "Whales" 
Users who joined in the last 2 weeks that will generate 
at least $500 in commission in the next 90 days 
 

 
7.5% 
(170 out of 
2270 
candidates) 
(2135 
Examples) 

 
37% 
21% 
 

 
28.8% 
27.2% 
 

 
21% 
19.6% 
 

Reducing activity 
Who among the current active users will reduce 
activity by 50% in the next 30 days (but will not churn) 
 

11.4% 
(255 out of 
2233 
candidates) 
(366 Examples) 

21% 
8% 
 
 

23% 
18.8% 
 

20.2% 
19.4% 
 

Churn in "Whales" 
Currently active "whales", who were active in the past 
week, to become inactive for the next 30 days? 

 

1.66% 
(69 out of 4141 
candidates) 
(21156 
Examples) 
 

10% 
11% 
 

10.8% 
12.4% 
 

9.2% 
8.4% 
 

Will trade in Apple for the first time 
Users who had never bought Apple share, and will buy 
it for the first time in the coming 30 days. 

 

0.5% 
(839 out of 
161382 
candidates) 
 

14% 
1% 
 

12% 
0.8% 
 

10% 
1% 
 

 

Observations: 

 Endor outperforms Tensor Flow in 3 out of 4 queries, and results in the same accuracy in the 4th. 

 The superiority of Endor is increasingly evident as the task becomes “more difficult” – focusing on 

the top-100 rather than the top-500. 

 There is a clear distinction between “less dynamic queries” (becoming a whale, churn, reduce 

activity” – for which static signals should likely be easier to detect) than the “Who will trade in 

Apple for the first time” query, which are (a) more dynamic, and (b) have a very low baseline, such 

that for the latter, Endor is X10 times more accurate! 

 As previously mentioned – the Tensor Flow results illustrated here employ 2-weeks of manual 

improvements done by a Deep Learning expert, whereas the Endor results are 100% automatic. 

 

  



_________________________________________________________________________ 
 

Appendix A : Social Physics Explained 

 

 
35     

 

 

 

3.2. AUTOMATICALLY CRACKING A KAGGLE CHALLENGE IN 3 HOURS 
 

In another example, we have tested Endor’s system with the publicly available data from the Kaggle 

competition known as “Acquire Valued Shoppers Challenge”. The data contained nearly 300 million 

point-of-sale records, referring to hundreds of thousands of customers, whereas the challenge entailed 

predicting which users who received a certain promotional coupon would become a recurring 

customer. 

 

The original challenge encompasses 952 teams and lasted 3 months.  

 

The Endor machine, running fully automatically on the raw data given to the challenge’s participants, 

was able to produce predictions that outperformed the team who originally won first place. 

 
 
 

 
 

 952 Competing data  
science teams 

 3 months challenge 
 

ENDOR:  
Few clicks, 1st Place 
 
 

 
 

 
 

 

 



Appendix B – Endor’s Common
Use-Cases for Enterprizes

What would it be like to have your own personal Oracle? What would it be like to have
access to the powerful data engines which only the likes of Google and Facebook exploit?
What would it be like if you could use the most powerful such engine to date Endor to get
the most reliable answer any available tech can give? With the vision to make Blockchain
prediction accessible and useful for all by democratizing advanced machine learning, En-
dor.coin empowers you with that ability, which until now was reserved only to technological
giants, equipped with internal teams of professional data-experts.

Following is a summary of the common prediction use-cases requested by Endor’s enter-
prize customers. It categorises the use-cases by the main segments Endor currently serves:

• Retail Banks

• Insurance Companies

• Retails and e-Commerce

However, thanks to the use of Social Physics, new use-cases can easily be supported – on
average requiring a few hours of an Endor’s sales engineer.

74



75



76



77



78



79



80



81



82



83



84



85



Appendix C – Endor.coin Examples
of Pre-Defined Predictions

While Endor.coin ’s grand vision aims at any sector being transformed by Blockchain, such as
Insurance, Banking, eCommerce or Health, to start with – we are offering an unprecedented
prediction platform for cryptocurrency insights to support cryptoholders seeking trading
leads. On Blockchains trust proven decentralized infrastructure, allowing anyone to test any
hypothesis, on any data source, without disclosing their actual trading strategies, Endor.coin
empowers you to see the future before it becomes observable through the lenses of any other
existing technologies.

Following is an example of some of the first pre-defined blockchain predictions to be
supported by the Endor.coin platform upon launch. New use-cases would gradually be
added, as requested by the community, using the RFP (Request for Predictions) mechanism.

Cryptocurrency Addresses Predictions: These predictions receives a pre-defined list
of addresses (i.e. “Bitcoin addresses that have had at least one outgoing transaction in
the past month”) and rank them according to their behavioral similarity to a pre-defined
behavior in the recent past (i.e. “addresses that have received at least 0.1 Bitcoin in the
past week”). The resulting list would contain at its top the addresses who most resemble the
pre-defined behavior (and therefore are statistically more likely to display the same behavior
in the future), whereas the bottom of the results list would contain addresses that least
resemble the pre-defined behavior.

• Active addresses: From all addresses who were active at least once in the past
month, which most resemble the addresses which significantly increased their number
of transactions recently?

• Heavy-trading addresses: From all addresses who were active at least once in the
past month, which most resemble the addresses whose overall transactions volume in
the past month passed 10 BTC?

• Becoming inactive addresses: From all addresses who were active at least once
in the past month, which most resemble the addresses which decreased their overall
transactions volume by 50% recently?

86



Token Predictions: These predictions receives a pre-defined list of token (i.e. “Tokens
that had at least $10M trading volume during the last month”) and rank them according
to their behavioral similarity to a pre-defined behavior in the recent past (i.e. “tokens that
increased their average monthly volume trading by 2”). The resulting list would have tokens
that most resemble the pre-defined behavior (and therefore are statistically more likely to
display the same behavior in the future) at the top, whereas tokens that least resemble the
pre-defined behavior would be places at the bottom of the list.

• Profitable tokens: From all tokens with at least $1M USD trading volume in the last
month, which ones most resemble tokens that have increased their price with respect
to BTC by over 50% in the last will month?

• Non-Profitable Tokens: From all tokens with at least $1M USD trading volume in
the last month, which ones most resemble tokens that have decreased their price with
respect to BTC by over 50% in the last will month?

• Volatile Tokens: From all tokens with at least $5M USD trading volume in the last
month, which ones most resemble the 10 most volatile tokens of last month?

• Stable Tokens: From all tokens with at least $5M USD trading volume in the last
month, which ones most resemble the 10 least volatile tokens of last month?

• Increasing Volume Tokens: From all tokens with at least $20M USD trading volume
in the last month, which ones most resemble tokens that have doubled their monthly
trading volume in the last will month?

• Decreasing Volume Tokens: From all tokens with at least $20M USD trading
volume in the last month, which ones most resemble tokens that have decreased their
monthly trading volume by 50% in the last will month?

87



Appendix D – Knowledge Sphere
Class: API Access

This Appendix contains the specification of the “Knowledge Sphere” data structure – the
basic building block of the Endor.coin protocol – as well as the complete description of the
class that provides access to it, for implementation of future analytics engines, to be plugged
to the Endor.coin network.

Usage Explanation: The clusters object consists of 3 entities:

• Sparse matrix M , of dimensions (|searchable Objects| × | Behavioral Clusters|), repre-
senting the connectivity between Searchable Objects and Behavioral Clusters, having
Mi,j = 1 iff searchable object of index i ∈ cluster j. Searchable objects refer to to-
kens, wallet addresses, locations, phone numbers – or any other type of objects that is
included in the data, and serve as the basis of the prediction.

• Array AM mapping each Searchable Object SO to an index in the sparse matrix M .

• DataFrame DM containing miscellaneous clusters properties, as defined and calculated
by each prediction engine. Such properties can be for example the size of the cluster,
ratio of internal connectivity vs. external connectivity, the internal module used by
the prediction engine to generate this cluster, and so on.

In order to extract and build the clusters object for usage, the following files are required:

• In order to build the sparse matrix M :
n ewFormat un i f i ed b l o ck s connec t i v i t y mat I rF i l e . spmat
newFormat un i f i ed b l o ck s connec t i v i ty matJcF i l e . spmat
newFormat un i f i ed b locks connect iv i ty matMtFi l e . spmat

• In order to build the mapping array AM :
newFormat TranslationTabletmp numbers . spmathlp

• In order to build clusters properties DataFrame DM :
newFormat un i f i ed b locks b lk data1 . mat

88



The above mentioned files are to be placed in a specific path, referred to by the cluster-
sExtractor class depicted below, as:

<c l u s t e r f i l e s p a t h >

Usage Example: After placing the relevant 5 clusters files in ¡cluster files path¿, clusters
can be extracted using the following code:

>>> e x t r a c t o r = c l u s t e r s E x t r a c t o r ( c l u s t e r f i l e s p a t h )
>>> pop to c lus te r s map , c l u s t e r s p r o p s , t r a n s l a t i o n p o p = e x t r a c t o r . r e t r i e v e C l u s t e r s ( )

In this example “pop to clusters map” points to the sparse matrix M . “clusters props”
points to the DataFrame DM containing the clusters properties. “translation pop” points to
the array mapping each searchable object to its index in the sparse matrix, AM .

The Clusters Extractor Class: Following is the complete description of the Knowledge Sphere
API:

import os
from cStr ingIO import Str ingIO
import s t r u c t
import t e m p f i l e
import numpy as np
import re
import s c ipy . i o as s c i o
import pandas as pd
from t e m p f i l e import NamedTemporaryFile
from sc ipy . spar s e import c s r mat r i x

c l a s s c l u s t e r s E x t r a c t o r ( ob j e c t ) :
de f i n i t ( s e l f , c l u s t e r f i l e s p a t h ) :

s e l f . path = c l u s t e r f i l e s p a t h
s e l f . mat in i t name = ’ blk data ’
s e l f . m a t f i e l d s = {

’ s rc ’ : 2 ,
’ type ’ : 1 ,
’ b lk type ’ : 1 ,
’ f i e l d ’ : 1 ,
’ f i e l d b y ’ : 1 ,
’N’ : 1 ,
’WN’ : 1 ,
’ deg ’ : 1 ,
’ thrs ’ : 2 ,
’SUB CLUSTERS FILE ’ : 1 ,
’ p e r c In t e rna l ’ : 1

}
de f get path ( s e l f ) :

r e turn s e l f . path
de f r e t r i e v e C l u s t e r s ( s e l f ) :

pop to c lu s t e r s map = s e l f . b u i l d p o p t o c l u s t e r s m a p ( )
c l u s t e r s p r o p s = s e l f . b u i l d m a t p r o p s d f ( )

89



t r a n s l a t i o n p o p = s e l f . g e t t r a n s l a t i o n p o p ( )
re turn pop to c lus te r s map , c l u s t e r s p r o p s , t r a n s l a t i o n p o p

de f b u i l d p o p t o c l u s t e r s m a p ( s e l f ) :
d ims for mat = s e l f . g e t d i m e n s i o n s f o r m a t r i x ( )
i n d i c e s = s e l f . g e t i r l i s t ( )
indptr = s e l f . g e t j c l i s t ( )
pop c luster map = s e l f . b u i l d p o p c l u s t m a t r i x ( dims for mat ,

i n d i c e s , indptr )
re turn pop c luster map

de f g e t d i m e n s i o n s f o r m a t r i x ( s e l f ) :
d ims f i l e name = ” MtFile . spmat”
a l l f i l e s = s e l f . g e t f i l e s i n p a t h ( d ims f i l e name )
f u l l p a t h = a l l f i l e s [ 0 ]
a = s e l f . open ( f u l l p a t h )
f = Str ingIO ( a . read ( ) )
mat s i z e s = s t r u c t . unpack( ’<2IQ ’ , f . read ( 1 6 ) )
re turn { ’ n rows ’ : mat s i z e s [ 0 ] , ’ n co l s ’ : mat s i z e s [ 1 ] ,

’ nnz ’ : mat s i z e s [ 2 ] }

de f g e t i r l i s t ( s e l f ) :
i r f i l e n a m e = ” I r F i l e . spmat”
a l l f i l e s = s e l f . g e t f i l e s i n p a t h ( i r f i l e n a m e )

f u l l p a t h = a l l f i l e s [ 0 ]
r e m o t e f i l e = s e l f . open ( f u l l p a t h )
data = r e m o t e f i l e . read (10 ∗ 1024 ∗ 1024)
loca l t emp path = os . path . j o i n ( t e m p f i l e . mkdtemp ( ) ,

i r f i l e n a m e )
with open ( loca l temp path , ’w’ ) as f :

whi l e data != ’ ’ :
f . wr i t e ( data )
data = r e m o t e f i l e . read (10 ∗ 1024 ∗ 1024)

i r = np . f r o m f i l e ( loca l temp path , dtype=np . in t32 )
os . un l ink ( loca l t emp path )
re turn i r

de f g e t j c l i s t ( s e l f ) :
j c f i l e n a m e = ” J c F i l e . spmat”

a l l f i l e s = s e l f . g e t f i l e s i n p a t h ( j c f i l e n a m e )

f u l l p a t h = a l l f i l e s [ 0 ]
data = s e l f . open ( f u l l p a t h )
l oca l t emp path = os . path . j o i n ( t e m p f i l e . mkdtemp ( ) , j c f i l e n a m e )
with open ( loca l temp path , ’w’ ) as f :

f . wr i t e ( data . read ( ) )

90



j c = np . f r o m f i l e ( loca l temp path , dtype=np . in t64 )
os . un l ink ( loca l t emp path )
re turn j c

de f b u i l d p o p c l u s t m a t r i x ( s e l f , mat dims , i n d i c e s , indptr ) :

nrows = mat dims [ ’ n rows ’ ]
n co l s = mat dims [ ’ n co l s ’ ]
nnz = mat dims [ ’ nnz ’ ]

data = np . ones ( nnz )
t ry :

mat = cs r mat r i x ( ( data , i n d i c e s , indptr ) ,
shape=(nco l s , nrows ) )

except Exception as e :
msg = ”””Couldn ’ t bu i ld populat ion to
c l u s t e r match due to : %s , abort ing .””” % s t r ( e )

r a i s e ValueError (msg)
re turn mat

de f b u i l d m a t p r o p s d f ( s e l f ) :
mat names l i s t = s e l f . g e t m a t f i l e s n a m e s ( )

i f l en ( mat names l i s t ) > 1 :
prop names = ’ temp ’
m a t f i l e s = [ s c i o . loadmat ( Str ingIO ( s e l f . open ( mat name ) . read ( ) ) )

f o r mat name in mat names l i s t ]
p rops d f = s e l f . b u i l d m u l t i p l e c l u s t e r s p r o p s ( m a t f i l e s , prop names )

e l s e :
prop names = ’ blk data ’
m a t f i l e = s c i o . loadmat ( Str ingIO ( s e l f . open ( mat names l i s t [ 0 ] ) . read ( ) ) )
p rops d f = s e l f . b u i l d s i n g l e c l u s t e r s p r o p s ( m a t f i l e , prop names )

re turn props d f

de f b u i l d m u l t i p l e c l u s t e r s p r o p s ( s e l f , m a t f i l e s , prop names ) :

a l l p r o p s d f = pd . DataFrame ( )
f o r m a t f i l e in m a t f i l e s :

d f = s e l f . b u i l d s i n g l e c l u s t e r s p r o p s ( m a t f i l e , prop names )
a l l p r o p s d f = a l l p r o p s d f . append ( df )

a l l p r o p s d f . index = range ( l en ( a l l p r o p s d f . index ) )
re turn a l l p r o p s d f

de f b u i l d s i n g l e c l u s t e r s p r o p s ( s e l f , m a t f i l e , prop names ) :
t ry :

c l u s t e r s p r o p s = m a t f i l e [ prop names ]
except Exception :

msg = ””” F i e ld %s doesn ’ t e x i s t in mat f i l e ,
but expected . Cannot cont inue ””” % prop names
r a i s e ValueError (msg)

91



c l u s t e r p r o p s = {}
f o r prop , counts in s e l f . m a t f i e l d s . i t e r i t e m s ( ) :

t ry :
mat values = c l u s t e r s p r o p s [ prop ] [ 0 ] [ 0 ]

except ( KeyError , IndexError ) :
ValueError (””” F i e ld %s doesn ’ t e x i s t in mat f i l e ,
p l e a s e remove i t from c o n f i g f i l e . ””” % prop )
break

i f counts == 1 :
c l u s t e r p r o p s [ prop ] = mat values . f l a t t e n ( )

e l s e :
f o r i in np . arange ( counts ) :

c l u s t e r p r o p s [ prop + ’ ’ + s t r ( i ) ] = mat values [ : , i ]
c l u s t e r p r o p d f = pd . DataFrame ( c l u s t e r p r o p s )
# no in spec t i on PyUnresolvedReferences
c l u s t e r p r o p d f . index . names = [ ’ c l u s t e r ’ ]
wn s i z e s = c l u s t e r p r o p d f [ ’WN’ ] . astype ( f l o a t )
n s i z e s = c l u s t e r p r o p d f [ ’N ’ ] . astype ( f l o a t )
c l u s t e r p r o p d f [ ’ W prcntg ’ ] = wn s i z e s / n s i z e s
de l m a t f i l e
r e turn c l u s t e r p r o p d f

de f g e t t r a n s l a t i o n p o p ( s e l f ) :
spmat help name = ” . spmathlp”
a l l f i l e s = s e l f . g e t f i l e s i n p a t h ( spmat help name )

f u l l p a t h = a l l f i l e s [ 0 ]

f spmath lp data = s e l f . open ( f u l l p a t h ) . read ( )
l oca l t emp path = os . path . j o i n ( t e m p f i l e . mkdtemp ( ) , spmat help name )
with open ( loca l temp path , ’w’ ) as f :

f . wr i t e ( f spmath lp data )
with open ( loca l temp path , ’ rb ’ ) as f spmathlp :

num = s t r u c t . unpack( ’<Q’ , f spmathlp . read ( 8 ) )
# no in spec t i on PyTypeChecker
i d s = np . f r o m f i l e ( f spmathlp , dtype=np . double )

i f num [ 0 ] != l en ( i d s ) :
msg = ””” t r a n s l a t i n g i d s went wrong . Found %d ids ,
where expected %d ids , abor t ing ””” % ( l en ( i d s ) , num [ 0 ] )
s e l f . l o g g e r . e r r o r (msg)
r a i s e ValueError (msg)

os . un l ink ( loca l t emp path )

re turn i d s

de f g e t f i l e s i n p a t h ( s e l f , name ) :
a l l f i l e s i n d i r = l i s t ( s e l f . l i s t d i r ( ) )

92



r e l e v a n t f i l e s = [ f i l e n a m e f o r f i l e n a m e in
a l l f i l e s i n d i r i f name in f i l e n a m e ]

re turn r e l e v a n t f i l e s

de f open ( s e l f , path ) :
r e a l p a t h = os . path . expanduser ( path )
i f not os . path . i s f i l e ( r e a l p a t h ) :

r a i s e LookupError ( ’”{}” does not ex i s t ’ . format ( r e a l p a t h ) )

re turn open ( rea l path , ’ rb ’ )

de f g e t m a t f i l e s n a m e s ( s e l f ) :
mat names l i s t = s e l f . g e t f i l e s i n p a t h ( s e l f . mat in i t name )
mat names l i s t . s o r t ( key=lambda x : i n t ( re . s earch ( r ’\d+ ’ , x ) . group ( ) ) )
re turn mat names l i s t

de f l i s t d i r ( s e l f ) :
r e turn [ os . path . j o i n ( s e l f . path , f ) f o r f in

os . l i s t d i r ( os . path . expanduser ( s e l f . path ) ) ]

93



Bibliography

[1] Wikipedia – Social Physics (2017).
URL https://en.wikipedia.org/wiki/Social_physics

[2] W. Pan, Y. Altshuler, A. Pentland, Decoding social influence and the wisdom of the crowd in financial
trading network, in: Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on
and 2012 International Confernece on Social Computing (SocialCom), IEEE, 2012, pp. 203–209.

[3] Y.-Y. Liu, J. C. Nacher, T. Ochiai, M. Martino, Y. Altshuler, Prospect theory for online financial
trading, PloS one 9 (10) (2014) e109458.

[4] Y. Altshuler, W. Pan, A. Pentland, Trends prediction using social diffusion models, arXiv.org, 2011.

[5] P. M. Krafft, J. Zheng, W. Pan, N. Della Penna, Y. Altshuler, E. Shmueli, J. B. Tenenbaum, A. Pentland,
Human collective intelligence as distributed bayesian inference, arXiv preprint arXiv:1608.01987.

[6] Y. Altshuler, A. S. Pentland, G. Gordon, Social behavior bias and knowledge management optimization,
in: Social Computing, Behavioral-Cultural Modeling, and Prediction, Springer, 2015, pp. 258–263.

[7] Y. Altshuler, A. Pentland, Methods and apparatus for tuning a network for optimal performance, uS
Patent 8,914,505 (Dec. 16 2014).
URL https://www.google.com/patents/US8914505

[8] W. Pan, Y. Altshuler, A. Pentland, N. Aharony, Methods and apparatus for prediction and modification
of behavior in networks, uS Patent 9,098,798 (Aug. 4 2015).
URL https://www.google.com/patents/US9098798

[9] Tuning social networks to gain the wisdom of the crowd (MIT Media Lab Website (2017).
URL https://www.media.mit.edu/research/highlights/tuning-social-networks-gain-wisdom-crowd

[10] Markets insight: Wake up to the twitter effect on markets (Financial Times) (2017).
URL http://web.media.mit.edu/~yanival/Markets_Insight.htm

[11] Beyond the echo chamber (Harvard Business Review) (2017).
URL https://hbr.org/2013/11/beyond-the-echo-chamber

[12] Rethinking predictive analytics (FirstMark’s Data Driven) (2017).
URL http://firstmarkcap.com/insights/rethinking-predictive-analytics/

[13] MIT s $1 million test to see if social media can make investors money (2013).
URL https://tinyurl.com/MIT-1M-USD

[14] Endor – inventing the “Google for Predictive Analytics” (2017).
URL http://news.mit.edu/2017/endor-inventing-google-predictive-analytics-1220

[15] Endor leading investor – Innovation Endeavors (2014).
URL http://www.innovationendeavors.com

94



[16] A. Boehme, Y. Altshuler, Using social physics to predict consumers behaviors, in: Network Science
(NetSci), 2017.

[17] Mastercard brings 5 new startups into start path accelerator program (2016).
URL https://tinyurl.com/MasterCard-Endor

[18] Endor – Finnovate Fall 2017 (2017).
URL https://www.youtube.com/watch?v=69rUQloq-qA

[19] Endor – a Gartner Cool Vendor (2017).
URL https://www.gartner.com/doc/3727117

[20] Endor – a technological pioneer acknowledgement of the World Economic Forum (2017).
URL http://widgets.weforum.org/techpioneers-2017/

[21] DARPA Network Challange (2011).
URL http://archive.darpa.mil/networkchallenge/

[22] The 2012 mckinsey award winners (2012).
URL https://hbr.org/2013/04/the-2012-mckinsey-award-winners

[23] Google Scholars – Professor Alex “Sandy” Pentland (2017).
URL https://scholar.google.com/citations?user=P4nfoKYAAAAJ&hl=en

[24] Tim Oreilly: the World’s 7 Most Powerful Data Scientists (2017).
URL http://www.forbes.com/pictures/lmm45emkh/6-alex-sandy-pentland-professor-mit/

[25] Y. Altshuler, A. Pentland, A. M. Bruckstein, Swarms and Network Intelligence in Search, Springer,
2017.

[26] Y. Altshuler, Y. Elovici, A. B. Cremers, N. Aharony, A. Pentland, Security and privacy in social
networks, Springer Science & Business Media, 2012.

[27] H. Shrobe, D. L. Shrier, A. Pentland, New Solutions for Cybersecurity, MIT Press, 2018.

[28] J. Clippinger, D. Bollier, From Bitcoin to Burning Man and Beyond: The Quest for Identity and
Autonomy in a Digital Society, ID3 and Off The Common Books, 2014.

[29] T. Hardjono, D. Shrier, A. Pentland, TRUST:: DATA: A New Framework for Identity and Data Sharing,
2016.

[30] A. Pentland, T. Heibeck, Honest signals: how they shape our world, MIT press, 2010.

[31] A. Pentland, Social physics: How good ideas spread-the lessons from a new science, Penguin, 2014.

[32] D. Shrier, A. Pentland, Frontiers of Financial Technology: Expeditions in future commerce, from
blockchain and digital banking to prediction markets and beyond, Publisher: Visionary Future, 2016.

[33] Endor.coin protocol GIT (2017).
URL https://github.com/orgs/EndorCoin

[34] Why you want blockchain-based ai, even if you dont know it yet (2017).
URL https://tinyurl.com/blockchain-based-AI

[35] Y. Altshuler, N. Aharony, A. Pentland, Y. Elovici, M. Cebrian, Stealing reality: When criminals become
data scientists (or vice versa), Intelligent Systems, IEEE 26 (6) (2011) 22–30. doi:10.1109/MIS.2011.78.

[36] ICOs: a Gold Mine or Fools Gold? (2017).
URL https://www.forbes.com/sites/outofasia/2017/12/07/icos-a-gold-mine-or-fools-gold/
#4482b6302cad

[37] M. Ulieru, Blockchain: what it is, how it really can change the world, World Economic Forum.

95



[38] How technology could help fix our broken financial system (2017).
URL https://tinyurl.com/technology-fixing-our-financia

[39] J. Pieprzyk, T. Hardjono, J. Seberry, Fundamentals of computer security, Springer Science & Business
Media, 2013.

[40] T. Hardjono, L. R. Dondeti, Multicast and group security, Artech House, 2003.

[41] T. Hardjono, L. R. Dondeti, Security in Wireless LANS and MANS (Artech House Computer Security),
Artech House, Inc., 2005.

[42] J. Seberry, T. Hardjono, Towards the Cryptanalysis of Bahasa Indonesia and Malaysia, 1989.

[43] S. G. Ong, J. Seberry, T. Hardjono, A. D. F. Academy., Towards the cryptanalysis of Mandarin (Pinyin),
1991.

DISCLAIMER: This White Paper is for discussion purpose only.
Endor.coin does not guarantee the accuracy of the conclusions reached in this white paper.
Copyright c© 2017 Endor.coin.

96


